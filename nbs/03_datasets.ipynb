{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7e8f88-8bca-4db0-b710-a4bc7259437b",
   "metadata": {},
   "source": [
    "# datasets\n",
    "\n",
    "> Utilities for dataset generation and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870ea3-a486-4d9b-960f-1fe61cbe59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598e2e6-bfbf-41b0-b491-40c3184eeba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93722153-82d9-4fa2-9800-d231a594c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import torch\n",
    "\n",
    "import esm\n",
    "\n",
    "\n",
    "_, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()\n",
    "\n",
    "\n",
    "def generate_dataset(parameters, msa_data, get_species_name, return_species=False):\n",
    "    \"\"\"\n",
    "    Function that given the two full paired MSAs of interacting sequences (seen as a list of tuples)\n",
    "    creates the dataset (dictionary of MSAs, both \"left\" and \"right\" ones), made of:\n",
    "\n",
    "    - \"msa\":   the MSA used to start the training of the permutation matrix.\n",
    "    - \"positive_examples\":  MSA of correct pairs to use as context during the training. It can be None\n",
    "                            if we don't want any context.\n",
    "\n",
    "    We have to specify if we either want the list of blocks or the positive examples by setting the value of\n",
    "    `generate_blocks` to True or False.\n",
    "\n",
    "    We can also limit the depth of the MSA by changing `limit_depth`.\n",
    "    Keep in mind that the maximum limit of sequences depends on the GPU memory.\n",
    "    \"\"\"\n",
    "    assert len(msa_data[0]) == len(msa_data[1])\n",
    "    dataset = {}\n",
    "\n",
    "    # Set random generators\n",
    "    rng = default_rng(seed=parameters[\"NUMPY_SEED\"])\n",
    "    rng_other = default_rng(seed=parameters[\"NUMPY_SEED_OTHER\"])\n",
    "    # Parameters of msa\n",
    "    N_init = parameters[\"N\"]\n",
    "    max_size_init = parameters[\"max_size\"]\n",
    "    # Count species in full MSA\n",
    "    species_l, inverse_l, counts_l = np.unique(\n",
    "        [get_species_name(rec[0]) for rec in msa_data[0]], return_inverse=True, return_counts=True\n",
    "    )\n",
    "    species_r, inverse_r, counts_r = np.unique(\n",
    "        [get_species_name(rec[0]) for rec in msa_data[1]], return_inverse=True, return_counts=True\n",
    "    )\n",
    "    assert set(species_l) == set(species_r), \"Species must be the same in the left and right MSA.\"\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # MAIN MSA\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Set positive_examples to None\n",
    "    dataset[\"positive_examples\"] = None\n",
    "    while True:\n",
    "        # Iterate until we find a collection of sequences with total depth\n",
    "        # 0.9 * N <= D <= 1.1 * N\n",
    "        idxs_shuffled = np.arange(len(species_l))\n",
    "        rng.shuffle(idxs_shuffled)\n",
    "        cumsum_counts_shuffled = np.cumsum(counts_l[idxs_shuffled])\n",
    "        idxs_in_range = np.flatnonzero(np.abs(cumsum_counts_shuffled - N_init) <= N_init * 0.1)\n",
    "        if len(idxs_in_range):\n",
    "            num_species = rng.choice(idxs_in_range) + 1\n",
    "            rand_species = np.sort(idxs_shuffled[:num_species])\n",
    "            counts_in_sample = counts_l[rand_species]\n",
    "            if np.all(counts_in_sample > 1) and np.all(counts_in_sample <= max_size_init):\n",
    "                break\n",
    "    # Create msa by concatenating the selected sequences\n",
    "    rand_idxs_l = []\n",
    "    rand_idxs_r = []\n",
    "    for unique_species_idx in rand_species:\n",
    "        rand_idxs_l += [i for i, label in enumerate(inverse_l) if label == unique_species_idx]\n",
    "        rand_idxs_r += [i for i, label in enumerate(inverse_r) if label == unique_species_idx]\n",
    "    dataset[\"msa\"] = {\n",
    "        \"left\": [msa_data[0][i] for i in rand_idxs_l],\n",
    "        \"right\": [msa_data[1][i] for i in rand_idxs_r]\n",
    "    }\n",
    "    # Print data\n",
    "    print(\"Generated initial MSA\")\n",
    "    print(\"\\tSpecies selected, total number of species selected:\")\n",
    "    print(species_l[rand_species])\n",
    "    print(rand_species, \",\", len(rand_species))\n",
    "    print(\"\\tPairs per species, total number of pairs:\")\n",
    "    print(counts_in_sample, \",\", sum(counts_in_sample))\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # POSITIVE EXAMPLES\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    if parameters[\"pos\"]:\n",
    "        while True:\n",
    "            # Indices of species not used in msa\n",
    "            unused_species_idxs = idxs_shuffled[num_species:].copy()\n",
    "            rng_other.shuffle(unused_species_idxs)\n",
    "            cumsum_counts_shuffled = np.cumsum(counts_l[unused_species_idxs])\n",
    "            # Iterate until we find a collection of sequences with total depth\n",
    "            # 0.9 * pos <= D <= 1.1 * pos\n",
    "            idxs_in_range_pos = np.flatnonzero(\n",
    "                np.abs(cumsum_counts_shuffled - parameters[\"pos\"]) <= parameters[\"pos\"] * 0.1\n",
    "            )\n",
    "            if len(idxs_in_range_pos):\n",
    "                num_species_pos = rng.choice(idxs_in_range_pos) + 1\n",
    "                rand_species_pos = np.sort(unused_species_idxs[:num_species_pos])\n",
    "                counts_in_sample_pos = counts_l[rand_species_pos]\n",
    "                if np.all(counts_in_sample_pos > 1):\n",
    "                    break\n",
    "        # Create msa of positive examples by concatenating the selected sequences\n",
    "        rand_idxs_pos_l = []\n",
    "        rand_idxs_pos_r = []\n",
    "        for unique_species_idx in rand_species_pos:\n",
    "            rand_idxs_pos_l += [\n",
    "                i for i, label in enumerate(inverse_l) if label == unique_species_idx\n",
    "            ]\n",
    "            rand_idxs_pos_r += [\n",
    "                i for i, label in enumerate(inverse_r) if label == unique_species_idx\n",
    "            ]\n",
    "        dataset[\"positive_examples\"] = {\n",
    "            \"left\": [msa_data[0][i] for i in rand_idxs_pos_l],\n",
    "            \"right\": [msa_data[1][i] for i in rand_idxs_rand_idxs_pos_rpos]\n",
    "        }\n",
    "        # Print data\n",
    "        print(\"\\n\\nGenerated positive examples\")\n",
    "        print(\"\\tSpecies selected, total number of species selected:\")\n",
    "        print(species_l[rand_species_pos])\n",
    "        print(rand_species_pos, \",\", len(rand_species_pos))\n",
    "        print(\"\\tPairs per species, total number of pairs:\")\n",
    "        print(counts_in_sample_pos, \",\", sum(counts_in_sample_pos))\n",
    "    else:\n",
    "        dataset[\"positive_examples\"] = None\n",
    "\n",
    "    if return_species:\n",
    "        return dataset, counts_in_sample, species_l[rand_species]\n",
    "    else:\n",
    "        return dataset, counts_in_sample\n",
    "\n",
    "\n",
    "def tokenizer(list_msa, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Function that given an MSA (seen as a list of tuples) tokenizes it using the MSA Transformer\n",
    "    tokenizer and transform the tokens into one-hot encodings.\n",
    "    \"\"\"\n",
    "    msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(list_msa)\n",
    "    tokens = torch.nn.functional.one_hot(\n",
    "        msa_batch_tokens, len(msa_alphabet.all_toks)\n",
    "    ).type(torch.FloatTensor).to(device)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def dataset_tokenizer(dataset, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Function that given a dictionary `dataset` of MSAs (initial MSA, blocks, positive examples) tokenizes\n",
    "    each MSA and return them in a dictionary with the same keys.\n",
    "    \"\"\"\n",
    "    dataset_tokens = {}\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Tokenize initial MSA\n",
    "        dataset_tokens[\"msa\"] = {key: tokenizer(dataset[\"msa\"][key], device=device)\n",
    "                                      for key in dataset[\"msa\"].keys()}\n",
    "        # Tokenize MSAs of positive examples and concatenate together the correct pairs, returns None\n",
    "        # if there are no positive examples\n",
    "        if dataset[\"positive_examples\"] is None:\n",
    "            dataset_tokens[\"positive_examples\"] = None\n",
    "        else:\n",
    "            tmp_pos_examples = {key: tokenizer(dataset[\"positive_examples\"][key], device=device)\n",
    "                                for key in dataset[\"positive_examples\"].keys()}\n",
    "            dataset_tokens[\"positive_examples\"] = torch.cat((tmp_pos_examples[\"left\"],\n",
    "                                                            tmp_pos_examples[\"right\"][..., 1:, :]), dim=2)\n",
    "\n",
    "        return dataset_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
