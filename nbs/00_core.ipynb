{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> DiffPALM class for optimizing MSA pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Torch and ESM\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# DiffPALM imports\n",
    "from diffpalm.gumbel_sinkhorn_utils import (\n",
    "    gumbel_sinkhorn, gumbel_matching, MSA_inverse_permutation, sample_uniform\n",
    ")\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import CenteredNorm\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "msa_transformer, _ = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_transformer = msa_transformer.eval()\n",
    "\n",
    "\n",
    "def DCN(x):\n",
    "    return x.detach().clone().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PermutationsMixin:\n",
    "    \"\"\"Mixin class for validating input and plotting.\"\"\"\n",
    "    mask_idx = 32\n",
    "\n",
    "    def _init_log_alpha(self, skip=False):\n",
    "        if not skip:\n",
    "            # Permutations restricted to species\n",
    "            self.log_alpha = [\n",
    "                (self.std_init * torch.randn(d, d, device=self.device)).requires_grad_(True)\n",
    "                for d in self._effective_depth_not_fixed\n",
    "            ]\n",
    "\n",
    "    def _validator(self, input_left, input_right, fixed_pairings=None):\n",
    "        # Validate input MSAs\n",
    "        depth_left, length_left, alphabet_size_left = input_left.shape[1:]\n",
    "        depth_right, length_right, alphabet_size_right = input_right.shape[1:]\n",
    "        length_left -= 1\n",
    "        length_right -= 1\n",
    "        if depth_left != depth_right:\n",
    "            raise ValueError(f\"Depth mismatch between left MSA ({depth_left}) and right MSA \"\n",
    "                             f\"({depth_right})\")\n",
    "        if alphabet_size_left != alphabet_size_right:\n",
    "            raise ValueError(\"Input MSAs must have the same alphabet size/\")\n",
    "        self._alphabet_size = alphabet_size_left\n",
    "\n",
    "        # Define oh vector for mask token\n",
    "        self._vec_mask = torch.zeros(self._alphabet_size, device=self.device)\n",
    "        self._vec_mask[self.mask_idx] = 1\n",
    "\n",
    "        # Validate depth attribute\n",
    "        self._depth_total = sum(self.species_sizes)\n",
    "        if depth_left != self._depth_total:\n",
    "            raise ValueError(f\"Input MSAs have depth {depth_left} but model expects a total \"\n",
    "                             f\"depth of {self._depth_total}\")\n",
    "        self._length_left, self._length_right = length_left, length_right\n",
    "        self._length = length_left + length_right\n",
    "\n",
    "        self._effective_mask_not_fixed = torch.ones(self._depth_total, self._depth_total,\n",
    "                                                    dtype=torch.bool, device=self.device)\n",
    "\n",
    "        # Create masking array for non-fixed partial rows in concatenated MSA\n",
    "        self._effective_mask_not_fixed_cat = torch.ones(\n",
    "            1, self._depth_total, self._length, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        if fixed_pairings is not None:\n",
    "            if len(fixed_pairings) != len(self.species_sizes):\n",
    "                raise ValueError(f\"`fixed_pairings` has length {len(fixed_pairings)} but \"\n",
    "                                 f\"there are {self.species_sizes} species.\")\n",
    "            _fixed_pairings = fixed_pairings\n",
    "\n",
    "            start = 0\n",
    "            self._effective_depth_not_fixed = []\n",
    "            self._effective_fixed_pairings_zip = []\n",
    "            for (species_idx,\n",
    "                 (species_size,\n",
    "                  species_fixed_pairings)) in enumerate(zip(self.species_sizes, _fixed_pairings)):\n",
    "                # Check uniqueness of pairs (i, j)\n",
    "                n_fixed = len(set(species_fixed_pairings))\n",
    "                if len(species_fixed_pairings) > n_fixed:\n",
    "                    raise ValueError(\"Repeated indices for fixed pairings at species \"\n",
    "                                     f\"{species_idx}: {species_fixed_pairings}\")\n",
    "                fixed_pairings_arr = np.zeros((species_size, species_size), dtype=int)\n",
    "                if species_fixed_pairings:\n",
    "                    species_fixed_pairings_zip = tuple(zip(*species_fixed_pairings))\n",
    "                else:\n",
    "                    # species_fixed_pairings is an empty list\n",
    "                    species_fixed_pairings_zip = (tuple(), tuple())\n",
    "                try:\n",
    "                    fixed_pairings_arr[species_fixed_pairings_zip] = 1\n",
    "                except IndexError:\n",
    "                    raise ValueError(\n",
    "                        f\"Fixed pairings indices out of bounds: passed {species_fixed_pairings} \"\n",
    "                        f\"for species {species_idx} with size {species_size}.\"\n",
    "                    )\n",
    "                partial_sum_0 = fixed_pairings_arr.sum(axis=0)\n",
    "                partial_sum_1 = fixed_pairings_arr.sum(axis=1)\n",
    "                if (partial_sum_0 > 1).any() or (partial_sum_1 > 1).any():\n",
    "                    raise ValueError(\n",
    "                        f\"Passed fixed pairings for species {species_idx} are either not one-one \"\n",
    "                        \"or a multiply-defined mapping from row to column indices: \"\n",
    "                        f\"{species_fixed_pairings}\"\n",
    "                    )\n",
    "                for i, j in species_fixed_pairings:\n",
    "                    self._effective_mask_not_fixed[start + i, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j] = False\n",
    "                total_minus_fixed = species_size - n_fixed\n",
    "                # If species_size - n_fixed <= 1 then actually everything is fixed\n",
    "                self._effective_depth_not_fixed.append(\n",
    "                    int(total_minus_fixed > 1) * total_minus_fixed\n",
    "                )\n",
    "                if total_minus_fixed == 1:\n",
    "                    # Deduce implicitly fixed pair\n",
    "                    i_implicit, j_implicit = np.argmin(partial_sum_1), np.argmin(partial_sum_0)\n",
    "                    self._effective_mask_not_fixed[start + i_implicit, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j_implicit] = False\n",
    "                    species_fixed_pairings_zip = (species_fixed_pairings_zip[0] + (i_implicit,),\n",
    "                                                  species_fixed_pairings_zip[1] + (j_implicit,))\n",
    "                self._effective_fixed_pairings_zip.append(species_fixed_pairings_zip)\n",
    "                start += species_size\n",
    "            start = 0\n",
    "            for (species_size,\n",
    "                 (rows_fixed, cols_fixed)) in zip(self.species_sizes, self._effective_fixed_pairings_zip):\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][:, rows_fixed, :length_left] = \\\n",
    "                    False\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][:, cols_fixed, length_left:] = \\\n",
    "                    False\n",
    "                start += species_size\n",
    "        else:\n",
    "            self._effective_depth_not_fixed = self.species_sizes\n",
    "            self._effective_fixed_pairings_zip = None\n",
    "\n",
    "        self._default_target_idx = torch.arange(\n",
    "            self._depth_total, dtype=torch.int64, device=self.device\n",
    "        )\n",
    "\n",
    "    def plot_real_time(\n",
    "        self,\n",
    "        it,\n",
    "        gs_matching_mat_np,\n",
    "        gs_mat_np,\n",
    "        list_idx,\n",
    "        target_idx,\n",
    "        list_log_alpha,\n",
    "        losses,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        lr,\n",
    "        tar_loss,\n",
    "        new_noise_factor,\n",
    "        output_dir,\n",
    "        only_loss_plot\n",
    "    ):\n",
    "        n_correct = [sum(idx == target_idx) for idx in list_idx[::batch_size]]\n",
    "\n",
    "        cmap = cm.get_cmap(\"Blues\")\n",
    "        normalizer = None\n",
    "        fig, axes = plt.subplots(figsize=(30, 5),\n",
    "                                 ncols=5,\n",
    "                                 constrained_layout=True)\n",
    "\n",
    "        null_model = 1\n",
    "        null_model = len(self.species_sizes)\n",
    "        _depth = [0] + list(np.cumsum(self.species_sizes))\n",
    "        for k in range(1, len(_depth)):\n",
    "            for ii in range(2):\n",
    "                elem, elem1 = _depth[k - 1], _depth[k]\n",
    "                axes[ii].plot([elem - .5, elem1 - .5, elem1 - .5, elem - .5],\n",
    "                              [elem - .5, elem - .5, elem1 - .5, elem1 - .5],\n",
    "                              color=\"r\")\n",
    "                axes[ii].plot([elem - .5, elem - .5, elem1 - .5, elem1 - .5],\n",
    "                              [elem - .5, elem1 - .5, elem1 - .5, elem - .5],\n",
    "                              color=\"r\")\n",
    "\n",
    "        ims_soft = axes[0].imshow(gs_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[0].set_title(f\"Soft {it // batch_size}\")\n",
    "        axes[1].imshow(gs_matching_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[1].set_title(\"Hard\")\n",
    "\n",
    "        curr_log_alpha = list_log_alpha[-1]\n",
    "        ims_log_alpha = axes[2].imshow(curr_log_alpha, norm=CenteredNorm(), cmap=cm.bwr)\n",
    "        axes[2].set_title(\"Log-alpha\")\n",
    "\n",
    "        prev_log_alpha = list_log_alpha[-2] if len(list_log_alpha) > 1 else curr_log_alpha\n",
    "        diff_log_alpha = curr_log_alpha - prev_log_alpha\n",
    "        if np.nansum(np.abs(diff_log_alpha)):\n",
    "            ims_log_alpha_diff = axes[3].imshow(diff_log_alpha, norm=CenteredNorm(), cmap=cm.bwr)\n",
    "            cb3 = fig.colorbar(ims_log_alpha_diff, ax=axes[3], shrink=0.8)\n",
    "        else:\n",
    "            ims_log_alpha_diff = axes[3].imshow(np.zeros_like(diff_log_alpha), cmap=cm.bwr)\n",
    "        axes[3].set_title(\"Log-alpha diff\")\n",
    "\n",
    "        avg_loss = np.mean(np.array(losses).reshape(-1, batch_size), axis=1)\n",
    "        axes[4].plot(avg_loss, color=\"b\", linewidth=1)\n",
    "        ax3_2 = None\n",
    "        if not only_loss_plot:\n",
    "            if tar_loss is not None:\n",
    "                axes[4].axhline(tar_loss, color=\"b\", linewidth=2)\n",
    "            diff = avg_loss[0] - tar_loss\n",
    "            axes[4].set_ylim([tar_loss - 0.6 * diff, avg_loss[0] + 0.5 * diff])\n",
    "            ax3_2 = axes[4].twinx()\n",
    "            ax3_2.set_ylabel(\"No. of correct pairs\", color=\"red\")\n",
    "            ax3_2.plot(n_correct, color=\"red\", linewidth=1)\n",
    "            ax3_2.axhline(null_model, color=\"red\", linewidth=2)\n",
    "            ax3_2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "        axes[4].set_ylabel(\"Loss\")\n",
    "        axes[4].set_xlim([0, epochs])\n",
    "        axes[4].set_title(f\"lr: {lr:.3g}, noise:{new_noise_factor:.3g}\")\n",
    "        fig.colorbar(ims_soft, ax=axes[0], shrink=0.8)\n",
    "        fig.colorbar(ims_log_alpha, ax=axes[2], shrink=0.8)\n",
    "        if output_dir is not None:\n",
    "            fig.savefig(output_dir / \"Iterations\" / f\"Epoch={it // batch_size}.svg\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DiffPALM(torch.nn.Module, PermutationsMixin):\n",
    "    \"\"\"\n",
    "    Class which permutes the pairs between two concatenated MSAs and backpropagate the loss on\n",
    "    the amino-acids directly to the permutation matrix to get the correct permutation of interacting\n",
    "    pairs. The input to the forward pass must be MSAs of one-hot encodings of shape (1,D,L,alphabet)\n",
    "    -- i.e., every amino-acid in the MSA is a one-hot vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        species_sizes,\n",
    "        *,\n",
    "        p_mask=0.7,\n",
    "        random_seed=42,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # List of species sizes for the paired MSA\n",
    "        self.species_sizes = species_sizes\n",
    "\n",
    "        # Token masking probability for left MSA\n",
    "        self.p_mask = p_mask\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Set random seed\n",
    "        self.random_seed = random_seed\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        # Set device and load MSA-Transformer to device\n",
    "        self.device = device\n",
    "        self.msa_tr = msa_transformer.to(self.device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ord,\n",
    "        input_right,\n",
    "        positive_examples=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the output logits of randomly masked tokens.\n",
    "\n",
    "        input_left: variable input (MSA to mask and permute)    --> (B, D, L1 + 1, 33)\n",
    "        input_right: fixed input (MSA of pairs: no masking)      --> (B, D, L2 + 1, 33)\n",
    "        positive_examples: if not None it's a concatenation of correct pairs to use as context\n",
    "                           (not masked)                        --> (B, D, L1 + L2 + 1, 33)\n",
    "        \"\"\"\n",
    "        # One-hot vector to use as mask in the MSA\n",
    "        vec_mask = torch.zeros(input_ord.shape[-1]).to(self.device)\n",
    "        vec_mask[self.mask_idx] = 1\n",
    "        # Create masking array # --> (B,D,L1)\n",
    "\n",
    "        # Mask input MSA in the positions specified by `mask` matrix (using`vec_mask`)\n",
    "        input_mask = input_ord.clone()\n",
    "        mask = (torch.rand(input_ord[..., 1:, 0].shape) < self.p_mask).to(self.device)\n",
    "\n",
    "        if self._effective_fixed_pairings_zip is not None:\n",
    "            start = 0\n",
    "            for (species_size,\n",
    "                 (_, cols_fixed)) in zip(self.species_sizes, self._effective_fixed_pairings_zip):\n",
    "                mask[:, start:, ...][:, cols_fixed, ...] = False\n",
    "                start += species_size\n",
    "\n",
    "        input_mask[..., 1:, :][mask] = vec_mask\n",
    "        # Concatenate masked MSA with fixed MSA (not masked)\n",
    "        input_mask = torch.cat((input_mask, input_right[..., 1:, :]), dim=2)\n",
    "\n",
    "        # Add positive examples (correct pairs) on top of input MSA\n",
    "        if positive_examples is not None:\n",
    "            input_mask = torch.cat((positive_examples, input_mask), dim=1)\n",
    "\n",
    "        # Compute output logits\n",
    "        results = msa_transformer(input_mask, repr_layers=[12])\n",
    "        logits = results[\"logits\"]\n",
    "        # Restrict to logits of masked (left) MSA, excluding positive examples\n",
    "        logits = logits[:, :, 1:input_ord.shape[2], :]\n",
    "        if positive_examples is not None:\n",
    "            logits = logits[:, positive_examples.shape[1]:, :, :]\n",
    "\n",
    "        return logits, mask\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        input_left,\n",
    "        input_right,\n",
    "        fixed_pairings=None,  # Format: list of lists of pairs of paired indices relative to each species [[(i, j), ...], ...]\n",
    "        positive_examples=None,\n",
    "        init_log_alpha=True,\n",
    "        std_init=0.1,\n",
    "        epochs=1,\n",
    "        optimizer_name=\"Adadelta\",\n",
    "        optimizer_kwargs=None,\n",
    "        tau=1.,\n",
    "        n_sink_iter=10,\n",
    "        noise=True,\n",
    "        noise_std=False,\n",
    "        noise_factor=0.1,\n",
    "        noise_scheduler=False,\n",
    "        scheduler_name=\"ReduceLROnPlateau\",\n",
    "        scheduler_kwargs=None,\n",
    "        batch_size=1,\n",
    "        use_rand_perm=False,\n",
    "        mean_centering=True,\n",
    "        tar_loss=None,\n",
    "        output_dir=None,\n",
    "        save_all_figs=False,\n",
    "        only_loss_plot=False\n",
    "    ):\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        if not sum(self._effective_depth_not_fixed):\n",
    "            print(\"No parameters available to optimize, pairings are fixed by the input.\")\n",
    "            return None\n",
    "        self.std_init = std_init\n",
    "\n",
    "        base_params = {\"noise\": noise, \"noise_std\": noise_std}\n",
    "        sinkhorn_params = {\"tau\": tau, \"n_iter\": n_sink_iter}\n",
    "\n",
    "        # Initialize log_alpha given fixed pairings\n",
    "        if init_log_alpha:\n",
    "            self._init_log_alpha()\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Useful functions\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        def _apply_species_wise(func):\n",
    "            # Apply `func` to the blocks for permutations restricted to species\n",
    "            def _impl(log_alpha, **params):\n",
    "                # Block matrix for permutations within species\n",
    "                noise_mat = params.pop(\"noise_mat\")  # List of noise matrices\n",
    "                rand_perm = params.pop(\"rand_perm\")  # List of random permutations\n",
    "                return torch.block_diag(\n",
    "                    *[func(la, noise_mat=nm, rand_perm=rp, **params) if la.size(0) else la\n",
    "                      for la, nm, rp in zip(log_alpha, noise_mat, rand_perm)]\n",
    "                )\n",
    "\n",
    "            return _impl\n",
    "\n",
    "        def _noise_mat():\n",
    "            if noise:\n",
    "                return [\n",
    "                    sample_uniform(la.size()).to(self.device) for la in self.log_alpha\n",
    "                ]\n",
    "\n",
    "            return [None for la in self.log_alpha]\n",
    "\n",
    "        def _rand_perm():\n",
    "            if use_rand_perm:\n",
    "                rand_perm = []\n",
    "                for la in self.log_alpha:\n",
    "                    n = la.shape[0]\n",
    "                    rp = []\n",
    "                    for _ in range(2):\n",
    "                        rp_i = torch.zeros_like(la, device=self.device)\n",
    "                        rp_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                        rp.append(rp_i)\n",
    "                    rand_perm.append(rp)\n",
    "            else:\n",
    "                rand_perm = [None] * len(self.log_alpha)\n",
    "\n",
    "            return rand_perm\n",
    "\n",
    "        gumbel_matching_species_wise = _apply_species_wise(gumbel_matching)\n",
    "        gumbel_sinkhorn_species_wise = _apply_species_wise(gumbel_sinkhorn)\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Input MSAs and initial variables\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        input_left = input_left.to(self.device).requires_grad_(True)\n",
    "        input_right = input_right.to(self.device).requires_grad_(True)\n",
    "\n",
    "        # Lists of parameters\n",
    "        losses = []\n",
    "        mats, mats_gs = [], []\n",
    "        list_idx = []\n",
    "        list_log_alpha = []\n",
    "        list_scheduler = []\n",
    "        gs_matching_mat = None\n",
    "        target_idx = torch.arange(self._depth_total, dtype=torch.float, device=self.device)\n",
    "        target_idx_np = DCN(target_idx)\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Initializations\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        # Optimizer\n",
    "        optimizer_params = [{\"params\": la} for la in self.log_alpha]\n",
    "        optimizer_kwargs_ = {} if optimizer_kwargs is None else deepcopy(optimizer_kwargs)\n",
    "        optimizer_cls = getattr(torch.optim, optimizer_name, torch.optim.SGD)\n",
    "        optimizer = optimizer_cls(optimizer_params, **optimizer_kwargs_)\n",
    "        # Scheduler\n",
    "        if scheduler_name is not None:\n",
    "            scheduler_cls = getattr(\n",
    "                    torch.optim.lr_scheduler, scheduler_name, torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "                )\n",
    "            scheduler = scheduler_cls(optimizer)\n",
    "\n",
    "        if output_dir is not None:\n",
    "            (output_dir / \"Iterations\").mkdir()\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Start training\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        iterations = epochs * batch_size\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "            for i in tqdm(range(iterations + batch_size)):\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Noise Matrices for permutations\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                if i % batch_size == 0:\n",
    "                    # Save log_alpha\n",
    "                    _log_alpha = torch.full((self._depth_total, self._depth_total), torch.nan,\n",
    "                                            dtype=torch.float, device=self.device)\n",
    "                    _log_alpha.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed,\n",
    "                        torch.block_diag(*self.log_alpha)\n",
    "                    )\n",
    "                    list_log_alpha.append(DCN(_log_alpha))\n",
    "                    # Create new noise matrices and random shufflings only every `batch_size` iterations\n",
    "                    noise_mat = _noise_mat()\n",
    "                    rand_perm = _rand_perm()\n",
    "                # Set value of noise correction\n",
    "                new_noise_factor = 0\n",
    "                if noise:\n",
    "                    new_noise_factor = noise_factor\n",
    "                    if noise_scheduler:\n",
    "                        new_noise_factor = \\\n",
    "                            noise_factor * optimizer.param_groups[0][\"lr\"] / optimizer_kwargs[\"lr\"]\n",
    "\n",
    "                # Mean-center log-alphas\n",
    "                if mean_centering:\n",
    "                    with torch.no_grad():\n",
    "                        for la in self.log_alpha:\n",
    "                            la[...] -= la.mean()\n",
    "\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute permutation matrices\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                params = {**base_params,\n",
    "                          **{\"noise_mat\": noise_mat, \"noise_factor\": new_noise_factor,\n",
    "                             \"rand_perm\": rand_perm}}\n",
    "                gs_matching_mat_not_fixed = gumbel_matching_species_wise(self.log_alpha, **params)\n",
    "                params.update(sinkhorn_params)\n",
    "                gs_mat_not_fixed = gumbel_sinkhorn_species_wise(self.log_alpha, **params)\n",
    "                if fixed_pairings is not None:\n",
    "                    gs_matching_mat = torch.zeros(self._depth_total, self._depth_total,\n",
    "                                                  dtype=torch.float, device=self.device)\n",
    "                    _gs_mat = torch.zeros_like(gs_matching_mat, device=self.device, requires_grad=True)\n",
    "                    gs_mat = _gs_mat.clone()\n",
    "                    start = 0\n",
    "                    for (species_size,\n",
    "                         species_fixed_pairings) in zip(self.species_sizes,\n",
    "                                                        self._effective_fixed_pairings_zip):\n",
    "                        gs_matching_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        gs_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        start += species_size\n",
    "                    gs_mat.masked_scatter_(self._effective_mask_not_fixed, gs_mat_not_fixed)\n",
    "                    gs_matching_mat.masked_scatter_(self._effective_mask_not_fixed,\n",
    "                                                    gs_matching_mat_not_fixed)\n",
    "                else:\n",
    "                    gs_matching_mat = gs_matching_mat_not_fixed\n",
    "                    gs_mat = gs_mat_not_fixed\n",
    "                # Save hard or soft permutation matrix\n",
    "                if i % batch_size == 0:\n",
    "                    mats.append(DCN(gs_matching_mat))\n",
    "                    mats_gs.append(DCN(gs_mat))\n",
    "                # Save permuted indexes\n",
    "                list_idx.append(DCN(torch.einsum(\"pq,p->q\", (gs_matching_mat, target_idx))))\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Permute sequences of input_left using detach trick to backprop only on soft perm.\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                input_left_ord = MSA_inverse_permutation(input_left, gs_mat)\n",
    "                input_left_ord_hard = MSA_inverse_permutation(input_left, gs_matching_mat)\n",
    "                input_left_ord = \\\n",
    "                    (input_left_ord_hard - input_left_ord).detach() + input_left_ord\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Get output logits of MSA Transformer for the permuted, masked input\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                loss = torch.tensor(0., device=self.device, requires_grad=True)\n",
    "                logits, mask = self(\n",
    "                    input_left_ord,\n",
    "                    input_right,\n",
    "                    positive_examples=positive_examples\n",
    "                )\n",
    "                loss = loss + self.loss(logits[mask], input_left_ord[..., 1:, :][mask])\n",
    "                loss = loss / batch_size\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute pseudoLikelihood loss only on masked positions and save gradients\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                pure_loss = loss.item()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Save loss values\n",
    "                losses.append(pure_loss * batch_size)\n",
    "                #      plot and save at every batch_size     or       no plots and save at last iteration\n",
    "                if (((i + 1) % batch_size == 0) and save_all_figs) or ((i == iterations + batch_size - 1) and not save_all_figs):\n",
    "                    self.plot_real_time(\n",
    "                        i, DCN(gs_matching_mat), DCN(gs_mat), list_idx, target_idx_np,\n",
    "                        list_log_alpha, losses, batch_size, epochs,\n",
    "                        optimizer.param_groups[0][\"lr\"], tar_loss, new_noise_factor,\n",
    "                        output_dir, only_loss_plot\n",
    "                    )\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Optimizer and Scheduler step (with gradient accumulation in batches)\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                # Compute this every time with exception of last iteration\n",
    "                if i < iterations:\n",
    "                    # Gradient Accumulation\n",
    "                    if ((i + 1) % batch_size == 0) or ((i + 1) == iterations):\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        # Update scheduler\n",
    "                        if scheduler_name is not None:\n",
    "                            if scheduler_name == \"ReduceLROnPlateau\":\n",
    "                                scheduler.step(sum(losses[-batch_size:]))\n",
    "                            else:\n",
    "                                scheduler.step()\n",
    "\n",
    "                        list_scheduler.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        return losses, list_scheduler, [target_idx_np, list_idx], [mats, mats_gs], list_log_alpha\n",
    "\n",
    "    def target_loss(\n",
    "        self,\n",
    "        input_left,\n",
    "        input_right,\n",
    "        fixed_pairings=None,\n",
    "        positive_examples=None,\n",
    "        batch_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function that computes the target value of the loss function using ordered pairs of input_left\n",
    "        and input_right.\n",
    "        `kind_loss` defines the loss function:\n",
    "            = \"average\" or None -> average loss over `batch_size` iterations\n",
    "            = \"per_sequence\" -> per-sequence average loss over `batch_size` iterations\n",
    "            = \"perplexity\" -> perplexity of the non-masked input concatenation.\n",
    "        \"\"\"\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        pbar = tqdm(range(batch_size))\n",
    "        pbar.set_description(\"Computing target loss\")\n",
    "\n",
    "        # Input MSAs\n",
    "        input_left = input_left.to(self.device)\n",
    "        input_right = input_right.to(self.device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            target_loss_val = []\n",
    "            torch.cuda.empty_cache()\n",
    "            for i in pbar:\n",
    "                logits, mask = self(\n",
    "                    input_left,\n",
    "                    input_right,\n",
    "                    positive_examples=positive_examples,\n",
    "                )\n",
    "                loss = self.loss(logits[mask], input_left[..., 1:, :][mask]).item()\n",
    "                target_loss_val.append(loss)\n",
    "\n",
    "        return target_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DiffPALM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
