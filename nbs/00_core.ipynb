{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> DiffPALM class for optimizing MSA pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Stdlib imports\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Torch and ESM\n",
    "import torch\n",
    "import esm\n",
    "\n",
    "# DiffPALM imports\n",
    "from diffpalm.gumbel_sinkhorn_utils import (\n",
    "    gumbel_sinkhorn,\n",
    "    gumbel_matching,\n",
    "    MSA_inverse_permutation,\n",
    "    sample_uniform,\n",
    ")\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import CenteredNorm\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "msa_transformer, _ = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_transformer = msa_transformer.eval()\n",
    "\n",
    "\n",
    "def DCN(x):\n",
    "    return x.detach().clone().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PermutationsMixin:\n",
    "    \"\"\"Mixin class for validating input and plotting the results of the optimization.\"\"\"\n",
    "\n",
    "    mask_idx = 32\n",
    "\n",
    "    def _init_log_alpha(self, skip=False):\n",
    "        \"\"\"Intialize log_alpha as a list of matrices of shape (d, d) where d is the\n",
    "        depth of the species MSA. The matrices are initialized with standard normal entries.\n",
    "        \"\"\"\n",
    "        if not skip:\n",
    "            # Permutations restricted to species\n",
    "            self.log_alpha = [\n",
    "                (self.std_init * torch.randn(d, d, device=self.device)).requires_grad_(\n",
    "                    True\n",
    "                )\n",
    "                for d in self._effective_depth_not_fixed\n",
    "            ]\n",
    "\n",
    "    def _validator(self, input_left, input_right, fixed_pairings=None):\n",
    "        \"\"\"Validate input MSAs and check fixed pairings.\"\"\"\n",
    "        # Validate input MSAs\n",
    "        depth_left, length_left, alphabet_size_left = input_left.shape[1:]\n",
    "        depth_right, length_right, alphabet_size_right = input_right.shape[1:]\n",
    "        length_left -= 1\n",
    "        length_right -= 1\n",
    "        if depth_left != depth_right:\n",
    "            raise ValueError(\n",
    "                f\"Depth mismatch between left MSA ({depth_left}) and right MSA \"\n",
    "                f\"({depth_right})\"\n",
    "            )\n",
    "        if alphabet_size_left != alphabet_size_right:\n",
    "            raise ValueError(\"Input MSAs must have the same alphabet size/\")\n",
    "        self._alphabet_size = alphabet_size_left\n",
    "\n",
    "        # Define oh vector for mask token\n",
    "        self._vec_mask = torch.zeros(self._alphabet_size, device=self.device)\n",
    "        self._vec_mask[self.mask_idx] = 1\n",
    "\n",
    "        # Validate depth attribute\n",
    "        self._depth_total = sum(self.species_sizes)\n",
    "        if depth_left != self._depth_total:\n",
    "            raise ValueError(\n",
    "                f\"Input MSAs have depth {depth_left} but model expects a total \"\n",
    "                f\"depth of {self._depth_total}\"\n",
    "            )\n",
    "        self._length_left, self._length_right = length_left, length_right\n",
    "        self._length = length_left + length_right\n",
    "\n",
    "        self._effective_mask_not_fixed = torch.ones(\n",
    "            self._depth_total, self._depth_total, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "\n",
    "        # Create masking array for non-fixed partial rows in concatenated MSA\n",
    "        self._effective_mask_not_fixed_cat = torch.ones(\n",
    "            1, self._depth_total, self._length, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        if fixed_pairings is not None:\n",
    "            if len(fixed_pairings) != len(self.species_sizes):\n",
    "                raise ValueError(\n",
    "                    f\"`fixed_pairings` has length {len(fixed_pairings)} but \"\n",
    "                    f\"there are {self.species_sizes} species.\"\n",
    "                )\n",
    "            _fixed_pairings = fixed_pairings\n",
    "\n",
    "            start = 0\n",
    "            self._effective_depth_not_fixed = []\n",
    "            self._effective_fixed_pairings_zip = []\n",
    "            for species_idx, (species_size, species_fixed_pairings) in enumerate(\n",
    "                zip(self.species_sizes, _fixed_pairings)\n",
    "            ):\n",
    "                # Check uniqueness of pairs (i, j)\n",
    "                n_fixed = len(set(species_fixed_pairings))\n",
    "                if len(species_fixed_pairings) > n_fixed:\n",
    "                    raise ValueError(\n",
    "                        \"Repeated indices for fixed pairings at species \"\n",
    "                        f\"{species_idx}: {species_fixed_pairings}\"\n",
    "                    )\n",
    "                fixed_pairings_arr = np.zeros((species_size, species_size), dtype=int)\n",
    "                if species_fixed_pairings:\n",
    "                    species_fixed_pairings_zip = tuple(zip(*species_fixed_pairings))\n",
    "                else:\n",
    "                    # species_fixed_pairings is an empty list\n",
    "                    species_fixed_pairings_zip = (tuple(), tuple())\n",
    "                try:\n",
    "                    fixed_pairings_arr[species_fixed_pairings_zip] = 1\n",
    "                except IndexError:\n",
    "                    raise ValueError(\n",
    "                        f\"Fixed pairings indices out of bounds: passed {species_fixed_pairings} \"\n",
    "                        f\"for species {species_idx} with size {species_size}.\"\n",
    "                    )\n",
    "                partial_sum_0 = fixed_pairings_arr.sum(axis=0)\n",
    "                partial_sum_1 = fixed_pairings_arr.sum(axis=1)\n",
    "                if (partial_sum_0 > 1).any() or (partial_sum_1 > 1).any():\n",
    "                    raise ValueError(\n",
    "                        f\"Passed fixed pairings for species {species_idx} are either not one-one \"\n",
    "                        \"or a multiply-defined mapping from row to column indices: \"\n",
    "                        f\"{species_fixed_pairings}\"\n",
    "                    )\n",
    "                for i, j in species_fixed_pairings:\n",
    "                    self._effective_mask_not_fixed[start + i, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j] = False\n",
    "                total_minus_fixed = species_size - n_fixed\n",
    "                # If species_size - n_fixed <= 1 then actually everything is fixed\n",
    "                self._effective_depth_not_fixed.append(\n",
    "                    int(total_minus_fixed > 1) * total_minus_fixed\n",
    "                )\n",
    "                if total_minus_fixed == 1:\n",
    "                    # Deduce implicitly fixed pair\n",
    "                    i_implicit, j_implicit = np.argmin(partial_sum_1), np.argmin(\n",
    "                        partial_sum_0\n",
    "                    )\n",
    "                    self._effective_mask_not_fixed[start + i_implicit, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j_implicit] = False\n",
    "                    species_fixed_pairings_zip = (\n",
    "                        species_fixed_pairings_zip[0] + (i_implicit,),\n",
    "                        species_fixed_pairings_zip[1] + (j_implicit,),\n",
    "                    )\n",
    "                self._effective_fixed_pairings_zip.append(species_fixed_pairings_zip)\n",
    "                start += species_size\n",
    "            start = 0\n",
    "            for species_size, (rows_fixed, cols_fixed) in zip(\n",
    "                self.species_sizes, self._effective_fixed_pairings_zip\n",
    "            ):\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][\n",
    "                    :, rows_fixed, :length_left\n",
    "                ] = False\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][\n",
    "                    :, cols_fixed, length_left:\n",
    "                ] = False\n",
    "                start += species_size\n",
    "        else:\n",
    "            self._effective_depth_not_fixed = self.species_sizes\n",
    "            self._effective_fixed_pairings_zip = None\n",
    "\n",
    "        self._default_target_idx = torch.arange(\n",
    "            self._depth_total, dtype=torch.int64, device=self.device\n",
    "        )\n",
    "\n",
    "    def plot_real_time(\n",
    "        self,\n",
    "        it,\n",
    "        gs_matching_mat_np,\n",
    "        gs_mat_np,\n",
    "        list_idx,\n",
    "        target_idx,\n",
    "        list_log_alpha,\n",
    "        losses,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        lr,\n",
    "        tar_loss,\n",
    "        new_noise_factor,\n",
    "        output_dir,\n",
    "        only_loss_plot,\n",
    "    ):\n",
    "        \"\"\"Plot the results of the optimization in real time.\"\"\"\n",
    "        n_correct = [sum(idx == target_idx) for idx in list_idx[::batch_size]]\n",
    "\n",
    "        cmap = cm.get_cmap(\"Blues\")\n",
    "        normalizer = None\n",
    "        fig, axes = plt.subplots(figsize=(30, 5), ncols=5, constrained_layout=True)\n",
    "\n",
    "        null_model = 1\n",
    "        null_model = len(self.species_sizes)\n",
    "        _depth = [0] + list(np.cumsum(self.species_sizes))\n",
    "        for k in range(1, len(_depth)):\n",
    "            for ii in range(2):\n",
    "                elem, elem1 = _depth[k - 1], _depth[k]\n",
    "                axes[ii].plot(\n",
    "                    [elem - 0.5, elem1 - 0.5, elem1 - 0.5, elem - 0.5],\n",
    "                    [elem - 0.5, elem - 0.5, elem1 - 0.5, elem1 - 0.5],\n",
    "                    color=\"r\",\n",
    "                )\n",
    "                axes[ii].plot(\n",
    "                    [elem - 0.5, elem - 0.5, elem1 - 0.5, elem1 - 0.5],\n",
    "                    [elem - 0.5, elem1 - 0.5, elem1 - 0.5, elem - 0.5],\n",
    "                    color=\"r\",\n",
    "                )\n",
    "\n",
    "        ims_soft = axes[0].imshow(gs_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[0].set_title(f\"Soft {it // batch_size}\")\n",
    "        axes[1].imshow(gs_matching_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[1].set_title(\"Hard\")\n",
    "\n",
    "        curr_log_alpha = list_log_alpha[-1]\n",
    "        ims_log_alpha = axes[2].imshow(curr_log_alpha, norm=CenteredNorm(), cmap=cm.bwr)\n",
    "        axes[2].set_title(\"Log-alpha\")\n",
    "\n",
    "        prev_log_alpha = (\n",
    "            list_log_alpha[-2] if len(list_log_alpha) > 1 else curr_log_alpha\n",
    "        )\n",
    "        diff_log_alpha = curr_log_alpha - prev_log_alpha\n",
    "        if np.nansum(np.abs(diff_log_alpha)):\n",
    "            ims_log_alpha_diff = axes[3].imshow(\n",
    "                diff_log_alpha, norm=CenteredNorm(), cmap=cm.bwr\n",
    "            )\n",
    "            cb3 = fig.colorbar(ims_log_alpha_diff, ax=axes[3], shrink=0.8)\n",
    "        else:\n",
    "            ims_log_alpha_diff = axes[3].imshow(\n",
    "                np.zeros_like(diff_log_alpha), cmap=cm.bwr\n",
    "            )\n",
    "        axes[3].set_title(\"Log-alpha diff\")\n",
    "\n",
    "        avg_loss = np.mean(np.array(losses).reshape(-1, batch_size), axis=1)\n",
    "        axes[4].plot(avg_loss, color=\"b\", linewidth=1)\n",
    "        ax3_2 = None\n",
    "        if not only_loss_plot:\n",
    "            if tar_loss is not None:\n",
    "                axes[4].axhline(tar_loss, color=\"b\", linewidth=2)\n",
    "            diff = avg_loss[0] - tar_loss\n",
    "            axes[4].set_ylim([tar_loss - 0.6 * diff, avg_loss[0] + 0.5 * diff])\n",
    "            ax3_2 = axes[4].twinx()\n",
    "            ax3_2.set_ylabel(\"No. of correct pairs\", color=\"red\")\n",
    "            ax3_2.plot(n_correct, color=\"red\", linewidth=1)\n",
    "            ax3_2.axhline(null_model, color=\"red\", linewidth=2)\n",
    "            ax3_2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "        axes[4].set_ylabel(\"Loss\")\n",
    "        axes[4].set_xlim([0, epochs])\n",
    "        axes[4].set_title(f\"lr: {lr:.3g}, noise:{new_noise_factor:.3g}\")\n",
    "        fig.colorbar(ims_soft, ax=axes[0], shrink=0.8)\n",
    "        fig.colorbar(ims_log_alpha, ax=axes[2], shrink=0.8)\n",
    "        if output_dir is not None:\n",
    "            fig.savefig(output_dir / \"Iterations\" / f\"Epoch={it // batch_size}.svg\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DiffPALM(torch.nn.Module, PermutationsMixin):\n",
    "    \"\"\"\n",
    "    Permute all the pairs between two concatenated MSAs (for each species), randomly mask the left MSA\n",
    "    and compute the MLM loss. Backpropagate the loss on the permutation matrix and iterate the process\n",
    "    to get the correct permutation of interacting pairs.\n",
    "    `species_sizes`: list of species sizes for the paired MSA\n",
    "    `p_mask`: token masking probability for left MSA\n",
    "    `random_seed`: random seed\n",
    "    `device`: device to use for computations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, species_sizes, *, p_mask=0.7, random_seed=42, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # List of species sizes for the paired MSA\n",
    "        self.species_sizes = species_sizes\n",
    "\n",
    "        # Token masking probability for left MSA\n",
    "        self.p_mask = p_mask\n",
    "\n",
    "        # Loss function\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Set random seed\n",
    "        self.random_seed = random_seed\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        # Set device and load MSA-Transformer to device\n",
    "        self.device = device\n",
    "        self.msa_tr = msa_transformer.to(self.device)\n",
    "\n",
    "    def forward(self, input_ord, input_right, positive_examples=None):\n",
    "        \"\"\"\n",
    "        Mask input MSA and concatenate with fixed MSA. Then compute output logits\n",
    "        for the masked positions using MSA-Transformer.\n",
    "\n",
    "        `input_ord`: variable input at each iteration (masked)  --> (B, D, L1 + 1, 33)\n",
    "        `input_right`: fixed input (no masking)                 --> (B, D, L2 + 1, 33)\n",
    "        `positive_examples`: if not None it's a concatenation of correct pairs to use\n",
    "                             as context (not masked)            --> (B, D, L1 + L2 + 1, 33)\n",
    "        \"\"\"\n",
    "        # One-hot vector to use as mask in the MSA\n",
    "        vec_mask = torch.zeros(input_ord.shape[-1]).to(self.device)\n",
    "        vec_mask[self.mask_idx] = 1\n",
    "        # Create masking array # --> (B,D,L1)\n",
    "\n",
    "        # Mask input MSA in the positions specified by `mask` matrix (using`vec_mask`)\n",
    "        input_mask = input_ord.clone()\n",
    "        mask = (torch.rand(input_ord[..., 1:, 0].shape) < self.p_mask).to(self.device)\n",
    "\n",
    "        if self._effective_fixed_pairings_zip is not None:\n",
    "            start = 0\n",
    "            for species_size, (_, cols_fixed) in zip(\n",
    "                self.species_sizes, self._effective_fixed_pairings_zip\n",
    "            ):\n",
    "                mask[:, start:, ...][:, cols_fixed, ...] = False\n",
    "                start += species_size\n",
    "\n",
    "        input_mask[..., 1:, :][mask] = vec_mask\n",
    "        # Concatenate masked MSA with fixed MSA (not masked)\n",
    "        input_mask = torch.cat((input_mask, input_right[..., 1:, :]), dim=2)\n",
    "\n",
    "        # Add positive examples (correct pairs) on top of input MSA\n",
    "        if positive_examples is not None:\n",
    "            input_mask = torch.cat((positive_examples, input_mask), dim=1)\n",
    "\n",
    "        # Compute output logits\n",
    "        results = msa_transformer(input_mask, repr_layers=[12])\n",
    "        logits = results[\"logits\"]\n",
    "        # Restrict to logits of masked (left) MSA, excluding positive examples\n",
    "        logits = logits[:, :, 1 : input_ord.shape[2], :]\n",
    "        if positive_examples is not None:\n",
    "            logits = logits[:, positive_examples.shape[1] :, :, :]\n",
    "\n",
    "        return logits, mask\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        input_left,\n",
    "        input_right,\n",
    "        fixed_pairings=None,  # Format: list of lists of pairs of paired indices relative to each species [[(i, j), ...], ...]\n",
    "        positive_examples=None,\n",
    "        init_log_alpha=True,\n",
    "        std_init=0.1,\n",
    "        epochs=1,\n",
    "        optimizer_name=\"Adadelta\",\n",
    "        optimizer_kwargs=None,\n",
    "        tau=1.0,\n",
    "        n_sink_iter=10,\n",
    "        noise=True,\n",
    "        noise_std=False,\n",
    "        noise_factor=0.1,\n",
    "        noise_scheduler=False,\n",
    "        scheduler_name=\"ReduceLROnPlateau\",\n",
    "        scheduler_kwargs=None,\n",
    "        batch_size=1,\n",
    "        use_rand_perm=False,\n",
    "        mean_centering=True,\n",
    "        tar_loss=None,\n",
    "        output_dir=None,\n",
    "        save_all_figs=False,\n",
    "        only_loss_plot=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the model using the input MSAs (`input_left`, `input_right`) and the fixed pairings.\n",
    "\n",
    "        `fixed_pairings`: list of lists of pairs of paired indices relative to each species\n",
    "                            Format: [[(i, j), ...], ...]\n",
    "        `init_log_alpha`: if True initialize log_alpha with random values\n",
    "        `std_init`: standard deviation of the normal distribution used to initialize log_alpha\n",
    "        `epochs`: number of epochs of the training\n",
    "        `optimizer_name`: name of the optimizer to use\n",
    "        `optimizer_kwargs`: kwargs of the optimizer\n",
    "        `tau`: temperature parameter for the Sinkhorn operator\n",
    "        `n_sink_iter`: number of Sinkhorn iterations\n",
    "        `noise`: if True add noise to the Gumbel-Matching algorithm\n",
    "        `noise_std`: if True use a fixed noise_std for the noise matrices\n",
    "        `noise_factor`: noise correction factor\n",
    "        `noise_scheduler`: if True use the optimizer learning rate to scale the noise_factor\n",
    "        `scheduler_name`: name of the learning rate scheduler to use\n",
    "        `scheduler_kwargs`: kwargs of the scheduler\n",
    "        `batch_size`: batch size for the training (number of different masks to use at each epoch)\n",
    "        `use_rand_perm`: if True use random permutations on the input MSAs to change the order of\n",
    "                         the sequences at each epoch\n",
    "        `mean_centering`: if True mean-center log_alphas at each epoch\n",
    "        `tar_loss`: if not None use this value as target loss for the training\n",
    "        `output_dir`: if not None save the plots in this directory\n",
    "        `save_all_figs`: if True save all the plots at each batch_size\n",
    "        `only_loss_plot`: if True save only the loss plot at each batch_size\n",
    "\n",
    "        Outputs:\n",
    "        `losses`: list of loss values for each iteration (`batch_size`*`epochs`)\n",
    "        `list_lr`: list of the learning rate used at each epoch\n",
    "        `list_idx`: list of the indexes of the predicted pairs at each iteration (`batch_size`*`epochs`)\n",
    "        `mats`: list of the permutation matrices at each epoch (hard permutation)\n",
    "        `mats_gs`: list of the soft-permutation matrices at each epoch\n",
    "        `list_log_alpha`: list of the log_alpha matrices at each epoch\n",
    "        \"\"\"\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        if not sum(self._effective_depth_not_fixed):\n",
    "            print(\n",
    "                \"No parameters available to optimize, pairings are fixed by the input.\"\n",
    "            )\n",
    "            return None\n",
    "        self.std_init = std_init\n",
    "\n",
    "        base_params = {\"noise\": noise, \"noise_std\": noise_std}\n",
    "        sinkhorn_params = {\"tau\": tau, \"n_iter\": n_sink_iter}\n",
    "\n",
    "        # Initialize log_alpha given fixed pairings\n",
    "        if init_log_alpha:\n",
    "            self._init_log_alpha()\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Useful functions\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        def _apply_species_wise(func):\n",
    "            # Apply `func` to the blocks for permutations restricted to species\n",
    "            def _impl(log_alpha, **params):\n",
    "                # Block matrix for permutations within species\n",
    "                noise_mat = params.pop(\"noise_mat\")  # List of noise matrices\n",
    "                rand_perm = params.pop(\"rand_perm\")  # List of random permutations\n",
    "                return torch.block_diag(\n",
    "                    *[\n",
    "                        func(la, noise_mat=nm, rand_perm=rp, **params)\n",
    "                        if la.size(0)\n",
    "                        else la\n",
    "                        for la, nm, rp in zip(log_alpha, noise_mat, rand_perm)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            return _impl\n",
    "\n",
    "        def _noise_mat():\n",
    "            if noise:\n",
    "                return [\n",
    "                    sample_uniform(la.size()).to(self.device) for la in self.log_alpha\n",
    "                ]\n",
    "\n",
    "            return [None for la in self.log_alpha]\n",
    "\n",
    "        def _rand_perm():\n",
    "            if use_rand_perm:\n",
    "                rand_perm = []\n",
    "                for la in self.log_alpha:\n",
    "                    n = la.shape[0]\n",
    "                    rp = []\n",
    "                    for _ in range(2):\n",
    "                        rp_i = torch.zeros_like(la, device=self.device)\n",
    "                        rp_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                        rp.append(rp_i)\n",
    "                    rand_perm.append(rp)\n",
    "            else:\n",
    "                rand_perm = [None] * len(self.log_alpha)\n",
    "\n",
    "            return rand_perm\n",
    "\n",
    "        gumbel_matching_species_wise = _apply_species_wise(gumbel_matching)\n",
    "        gumbel_sinkhorn_species_wise = _apply_species_wise(gumbel_sinkhorn)\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Input MSAs and initial variables\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        input_left = input_left.to(self.device).requires_grad_(True)\n",
    "        input_right = input_right.to(self.device).requires_grad_(True)\n",
    "\n",
    "        # Lists of parameters\n",
    "        losses = []\n",
    "        mats, mats_gs = [], []\n",
    "        list_idx = []\n",
    "        list_log_alpha = []\n",
    "        list_lr = []\n",
    "        gs_matching_mat = None\n",
    "        target_idx = torch.arange(\n",
    "            self._depth_total, dtype=torch.float, device=self.device\n",
    "        )\n",
    "        target_idx_np = DCN(target_idx)\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Initializations\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        # Optimizer\n",
    "        optimizer_params = [{\"params\": la} for la in self.log_alpha]\n",
    "        optimizer_kwargs_ = (\n",
    "            {} if optimizer_kwargs is None else deepcopy(optimizer_kwargs)\n",
    "        )\n",
    "        optimizer_cls = getattr(torch.optim, optimizer_name, torch.optim.SGD)\n",
    "        optimizer = optimizer_cls(optimizer_params, **optimizer_kwargs_)\n",
    "        # Scheduler\n",
    "        if scheduler_name is not None:\n",
    "            scheduler_cls = getattr(\n",
    "                torch.optim.lr_scheduler,\n",
    "                scheduler_name,\n",
    "                torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            )\n",
    "            scheduler = scheduler_cls(optimizer)\n",
    "\n",
    "        if output_dir is not None:\n",
    "            (output_dir / \"Iterations\").mkdir()\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Start training\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        iterations = epochs * batch_size\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "            for i in tqdm(range(iterations + batch_size)):\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Noise Matrices for permutations\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                if i % batch_size == 0:\n",
    "                    # Save log_alpha\n",
    "                    _log_alpha = torch.full(\n",
    "                        (self._depth_total, self._depth_total),\n",
    "                        torch.nan,\n",
    "                        dtype=torch.float,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "                    _log_alpha.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed,\n",
    "                        torch.block_diag(*self.log_alpha),\n",
    "                    )\n",
    "                    list_log_alpha.append(DCN(_log_alpha))\n",
    "                    # Create new noise matrices and random shufflings only every `batch_size` iterations\n",
    "                    noise_mat = _noise_mat()\n",
    "                    rand_perm = _rand_perm()\n",
    "                # Set value of noise correction\n",
    "                new_noise_factor = 0\n",
    "                if noise:\n",
    "                    new_noise_factor = noise_factor\n",
    "                    if noise_scheduler:\n",
    "                        new_noise_factor = (\n",
    "                            noise_factor\n",
    "                            * optimizer.param_groups[0][\"lr\"]\n",
    "                            / optimizer_kwargs[\"lr\"]\n",
    "                        )\n",
    "\n",
    "                # Mean-center log-alphas\n",
    "                if mean_centering:\n",
    "                    with torch.no_grad():\n",
    "                        for la in self.log_alpha:\n",
    "                            la[...] -= la.mean()\n",
    "\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute permutation matrices\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                params = {\n",
    "                    **base_params,\n",
    "                    **{\n",
    "                        \"noise_mat\": noise_mat,\n",
    "                        \"noise_factor\": new_noise_factor,\n",
    "                        \"rand_perm\": rand_perm,\n",
    "                    },\n",
    "                }\n",
    "                gs_matching_mat_not_fixed = gumbel_matching_species_wise(\n",
    "                    self.log_alpha, **params\n",
    "                )\n",
    "                params.update(sinkhorn_params)\n",
    "                gs_mat_not_fixed = gumbel_sinkhorn_species_wise(\n",
    "                    self.log_alpha, **params\n",
    "                )\n",
    "                if fixed_pairings is not None:\n",
    "                    gs_matching_mat = torch.zeros(\n",
    "                        self._depth_total,\n",
    "                        self._depth_total,\n",
    "                        dtype=torch.float,\n",
    "                        device=self.device,\n",
    "                    )\n",
    "                    _gs_mat = torch.zeros_like(\n",
    "                        gs_matching_mat, device=self.device, requires_grad=True\n",
    "                    )\n",
    "                    gs_mat = _gs_mat.clone()\n",
    "                    start = 0\n",
    "                    for species_size, species_fixed_pairings in zip(\n",
    "                        self.species_sizes, self._effective_fixed_pairings_zip\n",
    "                    ):\n",
    "                        gs_matching_mat[start:, start:][species_fixed_pairings] = 1.0\n",
    "                        gs_mat[start:, start:][species_fixed_pairings] = 1.0\n",
    "                        start += species_size\n",
    "                    gs_mat.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed, gs_mat_not_fixed\n",
    "                    )\n",
    "                    gs_matching_mat.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed, gs_matching_mat_not_fixed\n",
    "                    )\n",
    "                else:\n",
    "                    gs_matching_mat = gs_matching_mat_not_fixed\n",
    "                    gs_mat = gs_mat_not_fixed\n",
    "                # Save hard or soft permutation matrix\n",
    "                if i % batch_size == 0:\n",
    "                    mats.append(DCN(gs_matching_mat))\n",
    "                    mats_gs.append(DCN(gs_mat))\n",
    "                # Save permuted indexes\n",
    "                list_idx.append(\n",
    "                    DCN(torch.einsum(\"pq,p->q\", (gs_matching_mat, target_idx)))\n",
    "                )\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Permute sequences of input_left using detach trick to backprop only on soft perm.\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                input_left_ord = MSA_inverse_permutation(input_left, gs_mat)\n",
    "                input_left_ord_hard = MSA_inverse_permutation(\n",
    "                    input_left, gs_matching_mat\n",
    "                )\n",
    "                input_left_ord = (\n",
    "                    input_left_ord_hard - input_left_ord\n",
    "                ).detach() + input_left_ord\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Get output logits of MSA Transformer for the permuted, masked input\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                loss = torch.tensor(0.0, device=self.device, requires_grad=True)\n",
    "                logits, mask = self(\n",
    "                    input_left_ord, input_right, positive_examples=positive_examples\n",
    "                )\n",
    "                loss = loss + self.loss(logits[mask], input_left_ord[..., 1:, :][mask])\n",
    "                loss = loss / batch_size\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute pseudoLikelihood loss only on masked positions and save gradients\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                pure_loss = loss.item()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Save loss values\n",
    "                losses.append(pure_loss * batch_size)\n",
    "                #      plot and save at every batch_size     or       no plots and save at last iteration\n",
    "                if (((i + 1) % batch_size == 0) and save_all_figs) or (\n",
    "                    (i == iterations + batch_size - 1) and not save_all_figs\n",
    "                ):\n",
    "                    self.plot_real_time(\n",
    "                        i,\n",
    "                        DCN(gs_matching_mat),\n",
    "                        DCN(gs_mat),\n",
    "                        list_idx,\n",
    "                        target_idx_np,\n",
    "                        list_log_alpha,\n",
    "                        losses,\n",
    "                        batch_size,\n",
    "                        epochs,\n",
    "                        optimizer.param_groups[0][\"lr\"],\n",
    "                        tar_loss,\n",
    "                        new_noise_factor,\n",
    "                        output_dir,\n",
    "                        only_loss_plot,\n",
    "                    )\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Optimizer and Scheduler step (with gradient accumulation in batches)\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                # Compute this every time with exception of last iteration\n",
    "                if i < iterations:\n",
    "                    # Gradient Accumulation\n",
    "                    if ((i + 1) % batch_size == 0) or ((i + 1) == iterations):\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        # Update scheduler\n",
    "                        if scheduler_name is not None:\n",
    "                            if scheduler_name == \"ReduceLROnPlateau\":\n",
    "                                scheduler.step(sum(losses[-batch_size:]))\n",
    "                            else:\n",
    "                                scheduler.step()\n",
    "\n",
    "                        list_lr.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        return (\n",
    "            losses,\n",
    "            list_lr,\n",
    "            list_idx,\n",
    "            mats,\n",
    "            mats_gs,\n",
    "            list_log_alpha,\n",
    "        )\n",
    "\n",
    "    def target_loss(\n",
    "        self,\n",
    "        input_left,\n",
    "        input_right,\n",
    "        fixed_pairings=None,\n",
    "        positive_examples=None,\n",
    "        batch_size=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Function that computes the target value of the loss function using the pairs of `input_left`\n",
    "        and `input_right` ordered as in the input. The loss is computed using the MSA-Transformer.\n",
    "        `fixed_pairings`: list of lists of pairs of paired indices relative to each species\n",
    "        `positive_examples`: if not None it's a concatenation of correct pairs to use\n",
    "                             as context (not masked)\n",
    "        `batch_size`: batch size for the target loss (number of different masks to use at each epoch)\n",
    "\n",
    "        Output: list of target loss values for each masking iteration (`batch_size`)\n",
    "        \"\"\"\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        pbar = tqdm(range(batch_size))\n",
    "        pbar.set_description(\"Computing target loss\")\n",
    "\n",
    "        # Input MSAs\n",
    "        input_left = input_left.to(self.device)\n",
    "        input_right = input_right.to(self.device)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            target_loss_val = []\n",
    "            torch.cuda.empty_cache()\n",
    "            for i in pbar:\n",
    "                logits, mask = self(\n",
    "                    input_left,\n",
    "                    input_right,\n",
    "                    positive_examples=positive_examples,\n",
    "                )\n",
    "                loss = self.loss(logits[mask], input_left[..., 1:, :][mask]).item()\n",
    "                target_loss_val.append(loss)\n",
    "\n",
    "        return target_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DiffPALM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
