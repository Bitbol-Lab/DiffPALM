{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import esm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from torch.distributions import relaxed_bernoulli\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import CenteredNorm\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from DiffPALM.gumbel_sinkhorn_utils import *\n",
    "from DiffPALM.msa_transformer_utils import read_msa, read_sequence\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.device_count() == 1:\n",
    "        DEVICE = \"cuda\"\n",
    "    else:\n",
    "        device_no = input(\"Choose CUDA device number: \")\n",
    "        if device_no == \"\":\n",
    "            DEVICE = \"cpu\"\n",
    "        else:\n",
    "            DEVICE = f\"cuda:{device_no}\"\n",
    "# Uncomment when all needed ops are available for MPS\n",
    "# elif torch.backends.mps.is_built():\n",
    "#     DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "DEVICE = torch.device(DEVICE)\n",
    "print(f\"DEVICE = {DEVICE}\")\n",
    "\n",
    "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_transformer = msa_transformer.eval().to(DEVICE)\n",
    "msa_batch_converter = msa_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def DCN(x):\n",
    "    return x.detach().clone().cpu().numpy()\n",
    "\n",
    "\n",
    "class PermutationsMixin:\n",
    "    mask_idx = 32\n",
    "\n",
    "    def _init_log_alpha(self, skip=False):\n",
    "        if not skip:\n",
    "            if self._is_depth_ndarray:\n",
    "                # Permutations restricted to species\n",
    "                self.log_alpha = [\n",
    "                    (self.std_init * torch.randn(d, d, device=self.device)).requires_grad_(True)\n",
    "                    for d in self._effective_depth_not_fixed\n",
    "                ]\n",
    "            else:\n",
    "                # Permutations over all input sequences\n",
    "                d = self._effective_depth_not_fixed\n",
    "                self.log_alpha = (self.std_init * torch.randn(d, d, device=self.device)).requires_grad_(True)\n",
    "\n",
    "    def _validator(self, input_left, input_right, fixed_pairings=None):\n",
    "        self._is_depth_ndarray = type(self.depth) == np.ndarray\n",
    "        # Validate input MSAs\n",
    "        depth_left, length_left, alphabet_size_left = input_left.shape[1:]\n",
    "        depth_right, length_right, alphabet_size_right = input_right.shape[1:]\n",
    "        length_left -= 1\n",
    "        length_right -= 1\n",
    "        if depth_left != depth_right:\n",
    "            raise ValueError(f\"Depth mismatch between left MSA ({depth_left}) and right MSA \"\n",
    "                             f\"({depth_right})\")\n",
    "        if alphabet_size_left != alphabet_size_right:\n",
    "            raise ValueError(\"Input MSAs must have the same alphabet size/\")\n",
    "        self._alphabet_size = alphabet_size_left\n",
    "\n",
    "        # Define oh vector for mask token\n",
    "        self._vec_mask = torch.zeros(self._alphabet_size, device=self.device)\n",
    "        self._vec_mask[self.mask_idx] = 1\n",
    "\n",
    "        # Validate depth attribute\n",
    "        self._depth_total = self.depth if not self._is_depth_ndarray else sum(self.depth)\n",
    "        if depth_left != self._depth_total:\n",
    "            raise ValueError(f\"Input MSAs have depth {depth_left} but model expects a total \"\n",
    "                             f\"depth of {self._depth_total}\")\n",
    "        self._length_left, self._length_right = length_left, length_right\n",
    "        self._length = length_left + length_right\n",
    "\n",
    "        self._depth = self.depth if self._is_depth_ndarray else [self.depth]\n",
    "        self._effective_mask_not_fixed = torch.ones(self._depth_total, self._depth_total,\n",
    "                                                    dtype=torch.bool, device=self.device)\n",
    "\n",
    "        # Create masking array for non-fixed partial rows in concatenated MSA\n",
    "        self._effective_mask_not_fixed_cat = torch.ones(\n",
    "            1, self._depth_total, self._length, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        if fixed_pairings is not None:\n",
    "            if self._is_depth_ndarray:\n",
    "                if len(fixed_pairings) != len(self.depth):\n",
    "                    raise ValueError(f\"`fixed_pairings` has length {len(fixed_pairings)} but \"\n",
    "                                     f\"there are {self.depth} species.\")\n",
    "                _fixed_pairings = fixed_pairings\n",
    "            else:\n",
    "                _fixed_pairings = [fixed_pairings]\n",
    "\n",
    "            start = 0\n",
    "            self._effective_depth_not_fixed = []\n",
    "            self._effective_fixed_pairings_zip = []\n",
    "            for (species_idx,\n",
    "                 (species_size,\n",
    "                  species_fixed_pairings)) in enumerate(zip(self._depth, _fixed_pairings)):\n",
    "                # Check uniqueness of pairs (i, j)\n",
    "                n_fixed = len(set(species_fixed_pairings))\n",
    "                if len(species_fixed_pairings) > n_fixed:\n",
    "                    raise ValueError(\"Repeated indices for fixed pairings at species \"\n",
    "                                     f\"{species_idx}: {species_fixed_pairings}\")\n",
    "                fixed_pairings_arr = np.zeros((species_size, species_size), dtype=int)\n",
    "                if species_fixed_pairings:\n",
    "                    species_fixed_pairings_zip = tuple(zip(*species_fixed_pairings))\n",
    "                else:\n",
    "                    # species_fixed_pairings is an empty list\n",
    "                    species_fixed_pairings_zip = (tuple(), tuple())\n",
    "                try:\n",
    "                    fixed_pairings_arr[species_fixed_pairings_zip] = 1\n",
    "                except IndexError:\n",
    "                    raise ValueError(\n",
    "                        f\"Fixed pairings indices out of bounds: passed {species_fixed_pairings} \"\n",
    "                        f\"for species {species_idx} with size {species_size}.\"\n",
    "                    )\n",
    "                partial_sum_0 = fixed_pairings_arr.sum(axis=0)\n",
    "                partial_sum_1 = fixed_pairings_arr.sum(axis=1)\n",
    "                if (partial_sum_0 > 1).any() or (partial_sum_1 > 1).any():\n",
    "                    raise ValueError(\n",
    "                        f\"Passed fixed pairings for species {species_idx} are either not one-one \"\n",
    "                        \"or a multiply-defined mapping from row to column indices: \"\n",
    "                        f\"{species_fixed_pairings}\"\n",
    "                    )\n",
    "                for i, j in species_fixed_pairings:\n",
    "                    self._effective_mask_not_fixed[start + i, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j] = False\n",
    "                total_minus_fixed = species_size - n_fixed\n",
    "                # If species_size - n_fixed <= 1 then actually everything is fixed\n",
    "                self._effective_depth_not_fixed.append(\n",
    "                    int(total_minus_fixed > 1) * total_minus_fixed\n",
    "                )\n",
    "                if total_minus_fixed == 1:\n",
    "                    # Deduce implicitly fixed pair\n",
    "                    i_implicit, j_implicit = np.argmin(partial_sum_1), np.argmin(partial_sum_0)\n",
    "                    self._effective_mask_not_fixed[start + i_implicit, :] = False\n",
    "                    self._effective_mask_not_fixed[:, start + j_implicit] = False\n",
    "                    species_fixed_pairings_zip = (species_fixed_pairings_zip[0] + (i_implicit,),\n",
    "                                                  species_fixed_pairings_zip[1] + (j_implicit,))\n",
    "                self._effective_fixed_pairings_zip.append(species_fixed_pairings_zip)\n",
    "                start += species_size\n",
    "            start = 0\n",
    "            for (species_size,\n",
    "                 (rows_fixed, cols_fixed)) in zip(self._depth, self._effective_fixed_pairings_zip):\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][:, rows_fixed, :length_left] = \\\n",
    "                    False\n",
    "                self._effective_mask_not_fixed_cat[:, start:, ...][:, cols_fixed, length_left:] = \\\n",
    "                    False\n",
    "                start += species_size\n",
    "        else:\n",
    "            self._effective_depth_not_fixed = self._depth\n",
    "            self._effective_fixed_pairings_zip = None\n",
    "\n",
    "        self._default_target_idx = torch.arange(\n",
    "            self._depth_total, dtype=torch.int64, device=self.device\n",
    "        )\n",
    "\n",
    "    def plot_real_time(self,\n",
    "                       it,\n",
    "                       gs_matching_mat_np,\n",
    "                       gs_mat_np,\n",
    "                       list_idx,\n",
    "                       target_idx,\n",
    "                       list_log_alpha,\n",
    "                       losses,\n",
    "                       batch_size,\n",
    "                       epochs,\n",
    "                       lr,\n",
    "                       tar_loss,\n",
    "                       new_noise_factor,\n",
    "                       folder_path,\n",
    "                       showfig,\n",
    "                       only_loss_plot):\n",
    "        n_correct = [sum(idx == target_idx) for idx in list_idx[::batch_size]]\n",
    "\n",
    "        cmap = cm.get_cmap(\"Blues\")\n",
    "        normalizer = None\n",
    "        fig, axes = plt.subplots(figsize=(30, 5),\n",
    "                                 ncols=5,\n",
    "                                 constrained_layout=True)\n",
    "\n",
    "        null_model = 1\n",
    "        if self._is_depth_ndarray:\n",
    "            null_model = len(self.depth)\n",
    "            _depth = [0] + list(np.cumsum(self.depth))\n",
    "            for k in range(1, len(_depth)):\n",
    "                for ii in range(2):\n",
    "                    elem, elem1 = _depth[k - 1], _depth[k]\n",
    "                    axes[ii].plot([elem - .5, elem1 - .5, elem1 - .5, elem - .5],\n",
    "                                  [elem - .5, elem - .5, elem1 - .5, elem1 - .5],\n",
    "                                  color=\"r\")\n",
    "                    axes[ii].plot([elem - .5, elem - .5, elem1 - .5, elem1 - .5],\n",
    "                                  [elem - .5, elem1 - .5, elem1 - .5, elem - .5],\n",
    "                                  color=\"r\")\n",
    "\n",
    "        ims_soft = axes[0].imshow(gs_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[0].set_title(f\"Soft {it // batch_size}\")\n",
    "        axes[1].imshow(gs_matching_mat_np, cmap=cmap, norm=normalizer)\n",
    "        axes[1].set_title(\"Hard\")\n",
    "\n",
    "        curr_log_alpha = list_log_alpha[-1]\n",
    "        ims_log_alpha = axes[2].imshow(curr_log_alpha, norm=CenteredNorm(), cmap=cm.bwr)\n",
    "        axes[2].set_title(\"Log-alpha\")\n",
    "\n",
    "        prev_log_alpha = list_log_alpha[-2] if len(list_log_alpha) > 1 else curr_log_alpha\n",
    "        diff_log_alpha = curr_log_alpha - prev_log_alpha\n",
    "        if np.nansum(np.abs(diff_log_alpha)):\n",
    "            ims_log_alpha_diff = axes[3].imshow(diff_log_alpha, norm=CenteredNorm(), cmap=cm.bwr)\n",
    "            cb3 = fig.colorbar(ims_log_alpha_diff, ax=axes[3], shrink=0.8)\n",
    "        else:\n",
    "            ims_log_alpha_diff = axes[3].imshow(np.zeros_like(diff_log_alpha), cmap=cm.bwr)\n",
    "            cb3 = None\n",
    "        axes[3].set_title(\"Log-alpha diff\")\n",
    "\n",
    "        avg_loss = np.mean(np.array(losses).reshape(-1, batch_size), axis=1)\n",
    "        axes[4].plot(avg_loss, color=\"b\", linewidth=1)\n",
    "        ax3_2 = None\n",
    "        if not only_loss_plot:\n",
    "            if tar_loss is not None:\n",
    "                axes[4].axhline(tar_loss, color=\"b\", linewidth=2)\n",
    "            diff = avg_loss[0] - tar_loss\n",
    "            axes[4].set_ylim([tar_loss - 0.6 * diff, avg_loss[0] + 0.5 * diff])\n",
    "            ax3_2 = axes[4].twinx()\n",
    "            ax3_2.set_ylabel(\"No. of correct pairs\", color=\"red\")\n",
    "            ax3_2.plot(n_correct, color=\"red\", linewidth=1)\n",
    "            ax3_2.axhline(null_model, color=\"red\", linewidth=2)\n",
    "            ax3_2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "        axes[4].set_ylabel(\"Loss\")\n",
    "        axes[4].set_xlim([0, epochs])\n",
    "        axes[4].set_title(f\"lr: {lr:.3g}, noise:{new_noise_factor:.3g}\")\n",
    "        cb1 = fig.colorbar(ims_soft, ax=axes[0], shrink=0.8)\n",
    "        cb2 = fig.colorbar(ims_log_alpha, ax=axes[2], shrink=0.8)\n",
    "        if folder_path is not None:\n",
    "            if not folder_path.exists():\n",
    "                os.mkdir(folder_path)\n",
    "            fig.savefig(folder_path / f\"epoch={it // batch_size}.svg\")\n",
    "        if showfig:\n",
    "            plt.show()\n",
    "        # Clear the current axes.\n",
    "        plt.cla()\n",
    "        # Clear the current figure.\n",
    "        plt.clf()\n",
    "        # Closes all the figure windows.\n",
    "        plt.close('all')\n",
    "        plt.close(fig)\n",
    "        del fig, cb1, cb2, cb3, ax3_2, ims_soft, ims_log_alpha, ims_log_alpha_diff\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Permutations_MLM(torch.nn.Module, PermutationsMixin):\n",
    "    \"\"\"\n",
    "    Class which permutes the pairs between two concatenated MSAs and backpropagate the loss on\n",
    "    the amino-acids directly to the permutation matrix to get the correct permutation of interacting\n",
    "    pairs. The input to the forward pass must be MSAs of one-hot encodings of shape (1,D,L,alphabet)\n",
    "    -- i.e., every amino-acid in the MSA is a one-hot vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, depth, *, p_mask=0.7, only_mask_left=True, seed=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Depth of MSAs\n",
    "        self.depth = depth\n",
    "        # Masking probability of tokens\n",
    "        self.p_mask = p_mask\n",
    "        self.only_mask_left = only_mask_left\n",
    "        # Loss function\n",
    "        self.Loss = torch.nn.CrossEntropyLoss()\n",
    "        self.Loss_flat = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    def _validate_p_mask(self):\n",
    "        if self.only_mask_left or not isinstance(self.p_mask, list):\n",
    "            self._p_mask = [self.p_mask]  # Expected to be a single float or single tensor\n",
    "        else:  # Expect case of list of tensor\n",
    "            self._p_mask = self.p_mask\n",
    "\n",
    "    def forward(self, input_ord, input_right, positive_examples=None, p_mask=0.7,\n",
    "                offset_pos_emb_from=None, offset_pos_emb_by=0, only_masked_part=True):\n",
    "        \"\"\"\n",
    "        Compute the output logits of randomly masked tokens.\n",
    "\n",
    "        input_left: variable input (MSA to mask and permute)    --> (B, D, L1 + 1, 33)\n",
    "        input_right: fixed input (MSA of pairs: no masking)      --> (B, D, L2 + 1, 33)\n",
    "        positive_examples: if not None it's a concatenation of correct pairs to use as context\n",
    "                           (not masked)                        --> (B, D, L1 + L2 + 1, 33)\n",
    "        \"\"\"\n",
    "        # print(ii, torch.cuda.memory_allocated()/10**9, torch.cuda.memory_reserved()/10**9)\n",
    "\n",
    "        # One-hot vector to use as mask in the MSA\n",
    "        vec_mask = torch.zeros(input_ord.shape[-1]).to(self.device)\n",
    "        vec_mask[self.mask_idx] = 1\n",
    "        # Create masking array # --> (B,D,L1)\n",
    "\n",
    "        # Mask input MSA in the positions specified by `mask` matrix (using`vec_mask`)\n",
    "        if self.only_mask_left:\n",
    "            input_mask = input_ord.clone()\n",
    "            mask = (torch.rand(input_ord[..., 1:, 0].shape) < p_mask).to(self.device)\n",
    "        else:\n",
    "            input_mask = torch.cat((input_ord, input_right[..., 1:, :]), dim=2).clone()\n",
    "            mask = (torch.rand(input_mask[..., 1:, 0].shape) < p_mask).to(self.device)\n",
    "\n",
    "        if self._effective_fixed_pairings_zip is not None:\n",
    "            start = 0\n",
    "            for (species_size,\n",
    "                 (_, cols_fixed)) in zip(self._depth, self._effective_fixed_pairings_zip):\n",
    "                mask[:, start:, ...][:, cols_fixed, ...] = False\n",
    "                start += species_size\n",
    "\n",
    "        input_mask[..., 1:, :][mask] = vec_mask\n",
    "        if self.only_mask_left:\n",
    "            # Concatenate masked MSA with fixed MSA (not masked)\n",
    "            input_mask = torch.cat((input_mask, input_right[..., 1:, :]), dim=2)\n",
    "        # Add positive examples (correct pairs) on top of input MSA\n",
    "        if positive_examples is not None:\n",
    "            input_mask = torch.cat((positive_examples, input_mask), dim=1)\n",
    "\n",
    "        # Compute output logits\n",
    "        results = msa_transformer(input_mask, repr_layers=[12],\n",
    "                                  offset_pos_emb_from=offset_pos_emb_from,\n",
    "                                  offset_pos_emb_by=offset_pos_emb_by)\n",
    "        logits = results[\"logits\"]\n",
    "        if only_masked_part and self.only_mask_left:\n",
    "            # Restrict to logits of masked MSA\n",
    "            logits = logits[:, :, 1:input_ord.shape[2], :]\n",
    "        else:\n",
    "            # Output logits of both MSAs\n",
    "            logits = logits[:, :, 1:, :]\n",
    "        if positive_examples is not None:\n",
    "            logits = logits[:, positive_examples.shape[1]:, :, :]\n",
    "\n",
    "        return logits, mask\n",
    "\n",
    "    # @profile\n",
    "    def train(self,\n",
    "              input_left,\n",
    "              input_right,\n",
    "              fixed_pairings=None,  # [[(i, j), ...], ...]\n",
    "              positive_examples=None,\n",
    "              std_init=0.1,\n",
    "              epochs=1,\n",
    "              optimizer_name=\"Adam\",\n",
    "              optimizer_kwargs=None,\n",
    "              adaptive_weight_decay=False,\n",
    "              ord_reg=None,\n",
    "              lambda_reg=1e-3,\n",
    "              tau=1.,\n",
    "              n_sink_iter=20,\n",
    "              noise=True,\n",
    "              noise_std=False,\n",
    "              noise_factor=1.,\n",
    "              noise_scheduler=False,\n",
    "              scheduler_name=\"ReduceLROnPlateau\",\n",
    "              scheduler_kwargs=None,\n",
    "              batch_size=1,\n",
    "              use_rand_perm=False,\n",
    "              keep_working_hard_perm=False,\n",
    "              mean_centering=False,\n",
    "              hard_loss=False,\n",
    "              offset_pos_emb_from=None,\n",
    "              offset_pos_emb_by=0,\n",
    "              tar_loss=None,\n",
    "              folder_path=None,\n",
    "              time_processes=False,\n",
    "              save_checkpoint=False,\n",
    "              early_stopping=False,\n",
    "              showfig=False,\n",
    "              init_log_alpha=True,\n",
    "              only_loss_plot=False,\n",
    "              **kwargs):\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        if not sum(self._effective_depth_not_fixed):\n",
    "            print(\"No parameters available to optimize, pairings are fixed by the input.\")\n",
    "            return None\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self._validate_p_mask()\n",
    "\n",
    "        base_params = {\"noise\": noise, \"noise_std\": noise_std}\n",
    "        sinkhorn_params = {\"tau\": tau, \"n_iter\": n_sink_iter}\n",
    "\n",
    "        # Initialize log_alpha given fixed pairings\n",
    "        if init_log_alpha:\n",
    "            self._init_log_alpha()\n",
    "\n",
    "        if save_checkpoint and (folder_path is None):\n",
    "            raise ValueError(\"Cannot save checkpoint if `folder_path` is None.\")\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Useful functions\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        def _identity(x, **kwargs):\n",
    "            return x\n",
    "\n",
    "        def _impl_func(func):\n",
    "            # Apply `func` either to the whole log_alpha or just to the blocks for permutation over\n",
    "            # species\n",
    "            def _case_restricted_to_species(log_alpha, **params):\n",
    "                # Block matrix for permutations within species\n",
    "                noise_mat = params.pop(\"noise_mat\")  # List of noise matrices\n",
    "                rand_perm = params.pop(\"rand_perm\")  # List of random permutations\n",
    "                return torch.block_diag(\n",
    "                    *[func(la, noise_mat=nm, rand_perm=rp, **params) if la.size(0) else la\n",
    "                      for la, nm, rp in zip(log_alpha, noise_mat, rand_perm)]\n",
    "                )\n",
    "\n",
    "            def _case_unrestricted(log_alpha, **params):\n",
    "                # Else, permutation matrix over all sequences\n",
    "                # TODO INCLUDE NOISE AND RANDOM PERMUTATIONS\n",
    "                return func(log_alpha, **params) if log_alpha.size(0) else log_alpha\n",
    "\n",
    "            if self._is_depth_ndarray:\n",
    "                return _case_restricted_to_species\n",
    "            return _case_unrestricted\n",
    "\n",
    "        def _keep_working_hard_perm_func(gs_matching_mat, prev_gs_matching_mat=None):\n",
    "            # Keep same permutation if the frobenius norm of the previous one is bigger or\n",
    "            # equal than the new one (without Noise)\n",
    "            if prev_gs_matching_mat is None:\n",
    "                pass\n",
    "            elif self._is_depth_ndarray:\n",
    "                d_start = 0\n",
    "                for d, la in zip(self.depth, self.log_alpha):\n",
    "                    d_end = d_start + d\n",
    "                    gs_matching_submat = gs_matching_mat[d_start:d_end, d_start:d_end]\n",
    "                    prev_gs_matching_submat = prev_gs_matching_mat[d_start:d_end, d_start:d_end]\n",
    "                    frob = gs_matching_submat.T.matmul(la).diagonal().sum()\n",
    "                    prev_frob = prev_gs_matching_submat.T.matmul(la).diagonal().sum()\n",
    "                    if prev_frob >= frob:\n",
    "                        # Frobenius norm should be maximised, use old matching if not worse\n",
    "                        gs_matching_mat[d_start:d_end, d_start:d_end] = prev_gs_matching_submat\n",
    "                    d_start = d_end\n",
    "            else:\n",
    "                frob = gs_matching_mat.T.matmul(self.log_alpha).diagonal().sum()\n",
    "                prev_frob = prev_gs_matching_mat.T.matmul(self.log_alpha).diagonal().sum()\n",
    "                if prev_frob >= frob:\n",
    "                    # Frobenius norm should be maximised, use old matching if not worse\n",
    "                    gs_matching_mat[:, :] = prev_gs_matching_mat\n",
    "\n",
    "            return gs_matching_mat\n",
    "\n",
    "        def _noise_mat():\n",
    "            if self._is_depth_ndarray:\n",
    "                if noise:\n",
    "                    return [sample_uniform(la.size()).to(self.device) for la in self.log_alpha]\n",
    "                return [None for la in self.log_alpha]\n",
    "            elif noise:\n",
    "                return sample_uniform(self.log_alpha.size()).to(self.device)\n",
    "            return None\n",
    "\n",
    "        def _rand_perm():\n",
    "            if self._is_depth_ndarray:\n",
    "                if use_rand_perm:\n",
    "                    rand_perm = []\n",
    "                    for la in self.log_alpha:\n",
    "                        n = la.shape[0]\n",
    "                        rp = []\n",
    "                        for _ in range(2):\n",
    "                            rp_i = torch.zeros_like(la, device=self.device)\n",
    "                            rp_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                            rp.append(rp_i)\n",
    "                        rand_perm.append(rp)\n",
    "                else:\n",
    "                    rand_perm = [None] * len(self.log_alpha)\n",
    "            else:\n",
    "                if use_rand_perm:\n",
    "                    rand_perm = []\n",
    "                    n = self.log_alpha.shape[0]\n",
    "                    for _ in range(2):\n",
    "                        rand_perm_i = torch.zeros_like(self.log_alpha, device=self.device)\n",
    "                        rand_perm_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                        rand_perm.append(rand_perm_i)\n",
    "                else:\n",
    "                    rand_perm = None\n",
    "            return rand_perm\n",
    "\n",
    "        if keep_working_hard_perm and not noise:\n",
    "            processing_ = _keep_working_hard_perm_func\n",
    "        else:\n",
    "            processing_ = _identity\n",
    "\n",
    "        gumbel_matching_ = _impl_func(gumbel_matching)\n",
    "        gumbel_sinkhorn_ = _impl_func(gumbel_sinkhorn)\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Input MSAs and initial variables\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        input_left = input_left.to(self.device).requires_grad_(True)\n",
    "        input_right = input_right.to(self.device).requires_grad_(True)\n",
    "\n",
    "        if time_processes:\n",
    "            delta1, delta2, delta3 = [], [], []\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        # Lists of parameters\n",
    "        losses = []\n",
    "        mats, mats_gs = [], []\n",
    "        list_idx = []\n",
    "        list_log_alpha = []\n",
    "        list_scheduler = []\n",
    "        gs_matching_mat = None\n",
    "        target_idx = torch.arange(self._depth_total, dtype=torch.float, device=self.device)\n",
    "        target_idx_np = DCN(target_idx)\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Initializations\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        # Optimizer\n",
    "        if self._is_depth_ndarray:\n",
    "            optimizer_params = [{\"params\": la} for la in self.log_alpha]\n",
    "        else:\n",
    "            optimizer_params = [{\"params\": self.log_alpha}]\n",
    "        optimizer_kwargs_ = {} if optimizer_kwargs is None else deepcopy(optimizer_kwargs)\n",
    "        if adaptive_weight_decay:\n",
    "            # Divide the weight decay parameter with the square of the number of sequences\n",
    "            # in each block over ten.\n",
    "            wd = optimizer_kwargs_.pop(\"weight_decay\")\n",
    "            for elem in optimizer_params:\n",
    "                elem[\"weight_decay\"] = wd / (max(elem[\"params\"].shape[0], 1) / 10)**2\n",
    "        optimizer_cls = getattr(torch.optim, optimizer_name, torch.optim.SGD)\n",
    "        optimizer = optimizer_cls(optimizer_params, **optimizer_kwargs_)\n",
    "        # Scheduler\n",
    "        if scheduler_name is not None:\n",
    "            if type(scheduler_name) != list:\n",
    "                scheduler_name = [scheduler_name]\n",
    "                scheduler_kwargs = [scheduler_kwargs]\n",
    "            scheduler_ = {}\n",
    "            for elem, kwarg in zip(scheduler_name, scheduler_kwargs):\n",
    "                scheduler_cls = getattr(\n",
    "                    torch.optim.lr_scheduler, elem, torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "                )\n",
    "                scheduler_[elem] = scheduler_cls(optimizer, **kwarg)\n",
    "        else:\n",
    "            scheduler_cls = None\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Start training\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        iterations = epochs * batch_size\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "            for i in tqdm(range(iterations + batch_size)):\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Noise Matrices for permutations\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                if i % batch_size == 0:\n",
    "                    # Save log_alpha\n",
    "                    _log_alpha = torch.full((self._depth_total, self._depth_total), torch.nan,\n",
    "                                            dtype=torch.float, device=self.device)\n",
    "                    _log_alpha.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed,\n",
    "                        torch.block_diag(*self.log_alpha)\n",
    "                        if self._is_depth_ndarray else self.log_alpha\n",
    "                    )\n",
    "                    list_log_alpha.append(DCN(_log_alpha))\n",
    "                    # Create new noise matrices and random shufflings only every `batch_size` iterations\n",
    "                    noise_mat = _noise_mat()\n",
    "                    rand_perm = _rand_perm()\n",
    "                # Set value of noise correction\n",
    "                new_noise_factor = 0\n",
    "                if noise:\n",
    "                    new_noise_factor = noise_factor\n",
    "                    if noise_scheduler:\n",
    "                        new_noise_factor = \\\n",
    "                            noise_factor * optimizer.param_groups[0][\"lr\"] / optimizer_kwargs[\"lr\"]\n",
    "\n",
    "                # Timing\n",
    "                if time_processes:\n",
    "                    start.record()\n",
    "\n",
    "                # Mean-center log-alphas\n",
    "                if mean_centering:\n",
    "                    with torch.no_grad():\n",
    "                        for la in self.log_alpha:\n",
    "                            la[...] -= la.mean()\n",
    "\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute permutation matrices\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                params = {**base_params,\n",
    "                          **{\"noise_mat\": noise_mat, \"noise_factor\": new_noise_factor,\n",
    "                             \"rand_perm\": rand_perm}}\n",
    "                gs_matching_mat_not_fixed = processing_(gumbel_matching_(self.log_alpha, **params),\n",
    "                                                        prev_gs_matching_mat=gs_matching_mat)\n",
    "                params.update(sinkhorn_params)\n",
    "                gs_mat_not_fixed = gumbel_sinkhorn_(self.log_alpha, **params)\n",
    "                if fixed_pairings is not None:\n",
    "                    gs_matching_mat = torch.zeros(self._depth_total, self._depth_total,\n",
    "                                                  dtype=torch.float, device=self.device)\n",
    "                    _gs_mat = torch.zeros_like(gs_matching_mat, device=self.device, requires_grad=True)\n",
    "                    gs_mat = _gs_mat.clone()\n",
    "                    start = 0\n",
    "                    for (species_size,\n",
    "                         species_fixed_pairings) in zip(self._depth,\n",
    "                                                        self._effective_fixed_pairings_zip):\n",
    "                        gs_matching_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        gs_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        start += species_size\n",
    "                    gs_mat.masked_scatter_(self._effective_mask_not_fixed, gs_mat_not_fixed)\n",
    "                    gs_matching_mat.masked_scatter_(self._effective_mask_not_fixed,\n",
    "                                                    gs_matching_mat_not_fixed)\n",
    "                else:\n",
    "                    gs_matching_mat = gs_matching_mat_not_fixed\n",
    "                    gs_mat = gs_mat_not_fixed\n",
    "                # Save hard or soft permutation matrix\n",
    "                if i % batch_size == 0:\n",
    "                    mats.append(DCN(gs_matching_mat if hard_loss else gs_mat))\n",
    "                    mats_gs.append(DCN(gs_mat))\n",
    "                # Save permuted indexes\n",
    "                list_idx.append(DCN(torch.einsum(\"pq,p->q\", (gs_matching_mat, target_idx))))\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Permute sequences of input_left using detach trick to backprop only on soft perm.\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                input_left_ord = MSA_inverse_permutation(input_left, gs_mat)\n",
    "                if hard_loss or i == iterations:\n",
    "                    # Compute hard permutation (i == iterations to compute hard permutation also at\n",
    "                    # end of soft permutations)\n",
    "                    input_left_ord_hard = MSA_inverse_permutation(input_left, gs_matching_mat)\n",
    "                    if hard_loss:\n",
    "                        input_left_ord = \\\n",
    "                            (input_left_ord_hard - input_left_ord).detach() + input_left_ord\n",
    "                # Timing\n",
    "                if time_processes:\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    delta1.append(start.elapsed_time(end))\n",
    "                    start.record()\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Get output logits of MSA Transformer for the permuted, masked input\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                loss = torch.tensor(0., device=self.device, requires_grad=True)\n",
    "                input_for_CE = input_left_ord if self.only_mask_left else torch.cat((input_left_ord, input_right[..., 1:, :]), dim=2)\n",
    "                for p_mask in self._p_mask:\n",
    "                    logits, mask = self(\n",
    "                        input_left_ord, input_right, positive_examples=positive_examples,\n",
    "                        p_mask=p_mask, offset_pos_emb_from=offset_pos_emb_from,\n",
    "                        offset_pos_emb_by=offset_pos_emb_by\n",
    "                    )\n",
    "                    loss = loss + self.Loss(logits[mask], input_for_CE[..., 1:, :][mask])\n",
    "                loss = loss / batch_size\n",
    "                # Timing\n",
    "                if time_processes:\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    delta2.append(start.elapsed_time(end))\n",
    "                    start.record()\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute pseudoLikelihood loss only on masked positions and save gradients\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                pure_loss = loss.item()\n",
    "                # Add L_p regularization to the loss\n",
    "                if ord_reg is not None:\n",
    "                    loss_reg = torch.tensor(0., device=self.device, requires_grad=True)\n",
    "                    for ord in [ord_reg]:  # add , 2. if you want to implement L2 norm instead of weight decay\n",
    "                        if self._is_depth_ndarray:          # but then no weight_decay in optimizer\n",
    "                            for la in self.log_alpha:\n",
    "                                loss_reg = loss_reg + (la.exp() / tau).pow(ord).mean()\n",
    "                        else:\n",
    "                            loss_reg = loss_reg + (self.log_alpha.exp() / tau).pow(ord).mean()\n",
    "                    loss = loss + lambda_reg * loss_reg\n",
    "                # L1 - L2\n",
    "                # loss_reg = torch.tensor(0., device=self.device, requires_grad=True)\n",
    "                # for jjj in range(len(self.log_alpha)):\n",
    "                #     loss_reg = loss_reg + (torch.norm(self.log_alpha[jjj], p=1, dim=1) - torch.norm(self.log_alpha[jjj], p=2, dim=1)).sum() + (torch.norm(self.log_alpha[jjj], p=1, dim=0) - torch.norm(self.log_alpha[jjj], p=2, dim=0)).sum()\n",
    "                # loss = loss + lambda_reg * loss_reg\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Save loss values\n",
    "                losses.append(pure_loss * batch_size)\n",
    "                # Timing\n",
    "                if time_processes:\n",
    "                    end.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    delta3.append(start.elapsed_time(end))\n",
    "\n",
    "                #      plot and save at every batch_size     or       no plots and save at last iteration\n",
    "                if (((i + 1) % batch_size == 0) and showfig) or ((i == iterations + batch_size - 1) and not showfig):\n",
    "                    self.plot_real_time(\n",
    "                        i, DCN(gs_matching_mat), DCN(gs_mat), list_idx, target_idx_np,\n",
    "                        list_log_alpha, losses, batch_size, epochs,\n",
    "                        optimizer.param_groups[0][\"lr\"], tar_loss, new_noise_factor, folder_path,\n",
    "                        showfig, only_loss_plot\n",
    "                    )\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Optimizer and Scheduler step (with gradient accumulation in batches)\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                # Compute this every time with exception of last iteration\n",
    "                if i < iterations:\n",
    "                    # Gradient Accumulation\n",
    "                    if ((i + 1) % batch_size == 0) or ((i + 1) == iterations):\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        # Update scheduler\n",
    "                        if scheduler_name is not None:\n",
    "                            if len(scheduler_name) == 1:\n",
    "                                for elem in scheduler_name:\n",
    "                                    if elem == \"ReduceLROnPlateau\":\n",
    "                                        scheduler_[elem].step(sum(losses[-batch_size:]))\n",
    "                                    else:\n",
    "                                        scheduler_[elem].step()\n",
    "                            elif len(scheduler_name) == 2:\n",
    "                                if (i + 1) / iterations < scheduler_kwargs[0][\"pct_start\"]:\n",
    "                                    scheduler_[scheduler_name[0]].step()\n",
    "                                elif scheduler_name[1] == \"ReduceLROnPlateau\":\n",
    "                                    scheduler_[scheduler_name[1]].step(sum(losses[-batch_size:]))\n",
    "                                else:\n",
    "                                    scheduler_[scheduler_name[1]].step()\n",
    "                            else:\n",
    "                                raise NotImplementedError(\"No implementation for more than 2 \"\n",
    "                                                          \"schedulers\")\n",
    "\n",
    "                        list_scheduler.append(optimizer.param_groups[0][\"lr\"])\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Save checkpoint at every epoch\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                if save_checkpoint and ((i + 1) % batch_size == 0):\n",
    "                    checkpoint = {\n",
    "                        \"nb_epochs_finished\": (i + 1) // batch_size,\n",
    "                        \"log_alpha\": self.log_alpha,\n",
    "                        \"optimizer_state\": optimizer.state_dict(),\n",
    "                        \"scheduler_state\": (\n",
    "                            [scheduler_[elem].state_dict() for elem in scheduler_name]\n",
    "                            if scheduler_name is not None else None\n",
    "                        ),\n",
    "                        \"losses\": losses,\n",
    "                        \"list_scheduler\": list_scheduler,\n",
    "                        \"all_indices\": [target_idx_np, list_idx],\n",
    "                        \"list_perm_matrices\": mats,\n",
    "                        \"list_log_alphas\": list_log_alpha\n",
    "                    }\n",
    "                    torch.save(checkpoint, folder_path.parents[0] / \"checkpoint.pth\")\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Stopping rule\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                # If the permutation matrix is the same for (epochs/4) consecutive epochs, stops the\n",
    "                # training. It works only if there is noise, in all the non-noisy cases there are\n",
    "                # long periods in which the permutation matrix doesn't change (constant loss). Also\n",
    "                # first 50% of epochs is always trained, early stopping is possible only after that.\n",
    "                if early_stopping and noise and ((i + 1) % batch_size == 0) and ((i + 1) // batch_size >= epochs // 2):\n",
    "                    is_same_perm = [np.all(list_idx[-1] == list_idx[-batch_size * (i + 1)])\n",
    "                                    for i in range(epochs // 4)]\n",
    "                    if np.all(is_same_perm):\n",
    "                        print(f\"Early stopping at epoch: {(i + 1) // batch_size}\")\n",
    "                        break\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Show timing histograms\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        if time_processes:\n",
    "            fig, axes = plt.subplots(figsize=(10, 5),\n",
    "                                     ncols=3,\n",
    "                                     constrained_layout=True)\n",
    "            axes[0].hist(delta1, bins=20)\n",
    "            axes[0].set_title(\"Gumbel-Sinkhorn\")\n",
    "            axes[1].hist(delta2, bins=20)\n",
    "            axes[1].set_title(\"Forward\")\n",
    "            axes[2].hist(delta3, bins=20)\n",
    "            axes[2].set_title(\"Backward\")\n",
    "            fig.savefig(folder_path / \"time_processes.svg\")\n",
    "            plt.show()\n",
    "\n",
    "        return losses, list_scheduler, [target_idx_np, list_idx], [mats, mats_gs], list_log_alpha\n",
    "\n",
    "    def target_loss(self, input_left, input_right, fixed_pairings=None,\n",
    "                    positive_examples=None, batch_size=1, offset_pos_emb_from=None,\n",
    "                    offset_pos_emb_by=0, kind_loss=\"average\", **kwargs):\n",
    "        \"\"\"\n",
    "        Function that computes the target value of the loss function using ordered pairs of input_left\n",
    "        and input_right.\n",
    "        `kind_loss` defines the loss function:\n",
    "            = \"average\" or None -> average loss over `batch_size` iterations\n",
    "            = \"per_sequence\" -> per-sequence average loss over `batch_size` iterations\n",
    "            = \"perplexity\" -> perplexity of the non-masked input concatenation.\n",
    "        \"\"\"\n",
    "        if kind_loss != \"perplexity\":\n",
    "            self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "            pbar = tqdm(range(batch_size))\n",
    "            pbar.set_description(\"Computing target loss\")\n",
    "        else:\n",
    "            pbar = range(batch_size)\n",
    "        # Input MSAs\n",
    "        input_left = input_left.to(self.device)\n",
    "        input_right = input_right.to(self.device)\n",
    "\n",
    "        self._validate_p_mask()\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            target_loss_val = []\n",
    "            torch.cuda.empty_cache()\n",
    "            for i in pbar:\n",
    "                only_masked_part = True\n",
    "                if kind_loss == \"perplexity\":\n",
    "                    orig_p_mask = self.p_mask\n",
    "                    self.p_mask = 0\n",
    "                    only_masked_part = False\n",
    "                # logits, mask = self(\n",
    "                #     input_left, input_right,\n",
    "                #     positive_examples=positive_examples, offset_pos_emb_from=offset_pos_emb_from,\n",
    "                #     offset_pos_emb_by=offset_pos_emb_by, only_masked_part=only_masked_part\n",
    "                # )\n",
    "                if kind_loss == \"per_sequence\":\n",
    "                    loss_ = []\n",
    "                    for ind in range(logits.shape[1]):\n",
    "                        new_mask = mask[:, [ind], ...]\n",
    "                        ll = self.Loss(logits[:, [ind], ...][new_mask].reshape((-1, logits.shape[-1])),\n",
    "                                       input_left[:, [ind], 1:, :][new_mask].reshape((-1, input_left.shape[-1])))\n",
    "                        loss_.append(ll.item())\n",
    "                    loss_ = np.array(loss_)\n",
    "                    target_loss_val += [loss_]\n",
    "                elif kind_loss == \"perplexity\":\n",
    "                    loss = self.Loss_flat(logits.reshape((-1, logits.shape[-1])),\n",
    "                                          torch.cat((input_left[:, :, 1:, :],\n",
    "                                                     input_right[:, :, 1:, :]), dim=2).reshape((-1, input_left.shape[-1])))\n",
    "                    loss = DCN(loss)\n",
    "                    loss = loss.reshape((input_left.shape[1], -1))\n",
    "                    loss_ = loss.mean(1)\n",
    "                    loss_ = np.exp(loss_)\n",
    "                    target_loss_val += [loss_]\n",
    "                    # reset masking probability to original value\n",
    "                    self.p_mask = orig_p_mask\n",
    "                    # only one iteration is needed\n",
    "                    break\n",
    "                else:\n",
    "                    input_for_CE_loss = input_left if self.only_mask_left else torch.cat((input_left, input_right[..., 1:, :]), dim=2)\n",
    "                    for p_mask in self._p_mask:\n",
    "                        logits, mask = self(\n",
    "                            input_left, input_right, positive_examples=positive_examples,\n",
    "                            p_mask=p_mask, offset_pos_emb_from=offset_pos_emb_from,\n",
    "                            offset_pos_emb_by=offset_pos_emb_by, only_masked_part=only_masked_part\n",
    "                        )\n",
    "                        loss_ = self.Loss(logits[mask], input_for_CE_loss[..., 1:, :][mask]).item()\n",
    "                        target_loss_val += [loss_]\n",
    "\n",
    "        return target_loss_val\n",
    "\n",
    "    # @profile\n",
    "    def rw_train(self,\n",
    "              input_left,\n",
    "              input_right,\n",
    "              fixed_pairings=None,  # [[(i, j), ...], ...]\n",
    "              positive_examples=None,\n",
    "              std_init=0.1,\n",
    "              epochs=1,\n",
    "              tau=1.,\n",
    "              n_sink_iter=20,\n",
    "              batch_size=1,\n",
    "              use_rand_perm=False,\n",
    "              keep_working_hard_perm=False,\n",
    "              offset_pos_emb_from=None,\n",
    "              offset_pos_emb_by=0,\n",
    "              tar_loss=None,\n",
    "              folder_path=None,\n",
    "              showfig=False,\n",
    "              only_loss_plot=False,\n",
    "              **kwargs):\n",
    "        self._validator(input_left, input_right, fixed_pairings=fixed_pairings)\n",
    "        if not sum(self._effective_depth_not_fixed):\n",
    "            print(\"No parameters available to optimize, pairings are fixed by the input.\")\n",
    "            return None\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self._validate_p_mask()\n",
    "\n",
    "        base_params = {\"noise\": False, \"noise_mat\": None}\n",
    "        sinkhorn_params = {\"tau\": tau, \"n_iter\": n_sink_iter}\n",
    "\n",
    "        # Initialize log_alpha given fixed pairings\n",
    "        self._init_log_alpha()\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Useful functions\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        def _identity(x, **kwargs):\n",
    "            return x\n",
    "\n",
    "        def _noise_mat():\n",
    "            if self._is_depth_ndarray:\n",
    "                return [None for la in self.log_alpha]\n",
    "            return None\n",
    "        \n",
    "        def _impl_func(func):\n",
    "            # Apply `func` either to the whole log_alpha or just to the blocks for permutation over\n",
    "            # species\n",
    "            def _case_restricted_to_species(log_alpha, **params):\n",
    "                # Block matrix for permutations within species\n",
    "                noise_mat = params.pop(\"noise_mat\")  # List of noise matrices\n",
    "                rand_perm = params.pop(\"rand_perm\")  # List of random permutations\n",
    "                return torch.block_diag(\n",
    "                    *[func(la, noise_mat=nm, rand_perm=rp, **params) if la.size(0) else la\n",
    "                      for la, nm, rp in zip(log_alpha, noise_mat, rand_perm)]\n",
    "                )\n",
    "\n",
    "            def _case_unrestricted(log_alpha, **params):\n",
    "                # Else, permutation matrix over all sequences\n",
    "                # TODO INCLUDE NOISE AND RANDOM PERMUTATIONS\n",
    "                return func(log_alpha, **params) if log_alpha.size(0) else log_alpha\n",
    "\n",
    "            if self._is_depth_ndarray:\n",
    "                return _case_restricted_to_species\n",
    "            return _case_unrestricted\n",
    "\n",
    "        def _keep_working_hard_perm_func(gs_matching_mat, prev_gs_matching_mat=None):\n",
    "            # Keep same permutation if the frobenius norm of the previous one is bigger or\n",
    "            # equal than the new one (without Noise)\n",
    "            if prev_gs_matching_mat is None:\n",
    "                pass\n",
    "            elif self._is_depth_ndarray:\n",
    "                d_start = 0\n",
    "                for d, la in zip(self.depth, self.log_alpha):\n",
    "                    d_end = d_start + d\n",
    "                    gs_matching_submat = gs_matching_mat[d_start:d_end, d_start:d_end]\n",
    "                    prev_gs_matching_submat = prev_gs_matching_mat[d_start:d_end, d_start:d_end]\n",
    "                    frob = gs_matching_submat.T.matmul(la).diagonal().sum()\n",
    "                    prev_frob = prev_gs_matching_submat.T.matmul(la).diagonal().sum()\n",
    "                    if prev_frob >= frob:\n",
    "                        # Frobenius norm should be maximised, use old matching if not worse\n",
    "                        gs_matching_mat[d_start:d_end, d_start:d_end] = prev_gs_matching_submat\n",
    "                    d_start = d_end\n",
    "            else:\n",
    "                frob = gs_matching_mat.T.matmul(self.log_alpha).diagonal().sum()\n",
    "                prev_frob = prev_gs_matching_mat.T.matmul(self.log_alpha).diagonal().sum()\n",
    "                if prev_frob >= frob:\n",
    "                    # Frobenius norm should be maximised, use old matching if not worse\n",
    "                    gs_matching_mat[:, :] = prev_gs_matching_mat\n",
    "\n",
    "            return gs_matching_mat\n",
    "\n",
    "        def _rand_perm():\n",
    "            if self._is_depth_ndarray:\n",
    "                if use_rand_perm:\n",
    "                    rand_perm = []\n",
    "                    for la in self.log_alpha:\n",
    "                        n = la.shape[0]\n",
    "                        rp = []\n",
    "                        for _ in range(2):\n",
    "                            rp_i = torch.zeros_like(la, device=self.device)\n",
    "                            rp_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                            rp.append(rp_i)\n",
    "                        rand_perm.append(rp)\n",
    "                else:\n",
    "                    rand_perm = [None] * len(self.log_alpha)\n",
    "            else:\n",
    "                if use_rand_perm:\n",
    "                    rand_perm = []\n",
    "                    n = self.log_alpha.shape[0]\n",
    "                    for _ in range(2):\n",
    "                        rand_perm_i = torch.zeros_like(self.log_alpha, device=self.device)\n",
    "                        rand_perm_i[torch.arange(n), torch.randperm(n)] = 1\n",
    "                        rand_perm.append(rand_perm_i)\n",
    "                else:\n",
    "                    rand_perm = None\n",
    "            return rand_perm\n",
    "\n",
    "        if keep_working_hard_perm:\n",
    "            processing_ = _keep_working_hard_perm_func\n",
    "        else:\n",
    "            processing_ = _identity\n",
    "\n",
    "        gumbel_matching_ = _impl_func(gumbel_matching)\n",
    "        gumbel_sinkhorn_ = _impl_func(gumbel_sinkhorn)\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Input MSAs and initial variables\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        input_left = input_left.to(self.device).requires_grad_(False)\n",
    "        input_right = input_right.to(self.device).requires_grad_(False)\n",
    "\n",
    "        # Lists of parameters\n",
    "        losses = []\n",
    "        mats, mats_gs = [], []\n",
    "        list_idx = []\n",
    "        list_log_alpha = []\n",
    "        gs_matching_mat = None\n",
    "        target_idx = torch.arange(self._depth_total, dtype=torch.float, device=self.device)\n",
    "        target_idx_np = DCN(target_idx)\n",
    "\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        ## Start training\n",
    "        # ------------------------------------------------------------------------------------------\n",
    "        iterations = epochs * batch_size\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(iterations + batch_size)):\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Block Matrices for permutations in species\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                if i % batch_size == 0:\n",
    "                    # Save log_alpha\n",
    "                    _log_alpha = torch.full((self._depth_total, self._depth_total), torch.nan,\n",
    "                                            dtype=torch.float, device=self.device)\n",
    "                    _log_alpha.masked_scatter_(\n",
    "                        self._effective_mask_not_fixed,\n",
    "                        torch.block_diag(*self.log_alpha)\n",
    "                        if self._is_depth_ndarray else self.log_alpha\n",
    "                    )\n",
    "                    list_log_alpha.append(DCN(_log_alpha))\n",
    "                    # Random shufflings only every `batch_size` iterations\n",
    "                    rand_perm = _rand_perm()\n",
    "                    noise_mat = _noise_mat()\n",
    "                # Set value of noise correction\n",
    "                new_noise_factor = 0\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Compute permutation matrices\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                params = {**base_params,\n",
    "                          **{\"noise_mat\": noise_mat, \"noise_factor\": new_noise_factor,\n",
    "                             \"rand_perm\": rand_perm}}\n",
    "                gs_matching_mat_not_fixed = processing_(gumbel_matching_(self.log_alpha, **params),\n",
    "                                                        prev_gs_matching_mat=gs_matching_mat)\n",
    "                params.update(sinkhorn_params)\n",
    "                gs_mat_not_fixed = gumbel_sinkhorn_(self.log_alpha, **params)\n",
    "                if fixed_pairings is not None:\n",
    "                    gs_matching_mat = torch.zeros(self._depth_total, self._depth_total,\n",
    "                                                  dtype=torch.float, device=self.device)\n",
    "                    _gs_mat = torch.zeros_like(gs_matching_mat, device=self.device, requires_grad=True)\n",
    "                    gs_mat = _gs_mat.clone()\n",
    "                    start = 0\n",
    "                    for (species_size,\n",
    "                         species_fixed_pairings) in zip(self._depth,\n",
    "                                                        self._effective_fixed_pairings_zip):\n",
    "                        gs_matching_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        gs_mat[start:, start:][species_fixed_pairings] = 1.\n",
    "                        start += species_size\n",
    "                    gs_mat.masked_scatter_(self._effective_mask_not_fixed, gs_mat_not_fixed)\n",
    "                    gs_matching_mat.masked_scatter_(self._effective_mask_not_fixed,\n",
    "                                                    gs_matching_mat_not_fixed)\n",
    "                else:\n",
    "                    gs_matching_mat = gs_matching_mat_not_fixed\n",
    "                    gs_mat = gs_mat_not_fixed\n",
    "                # Save hard or soft permutation matrix\n",
    "                if i % batch_size == 0:\n",
    "                    mats.append(DCN(gs_matching_mat))\n",
    "                    mats_gs.append(DCN(gs_mat))\n",
    "                # Save permuted indexes\n",
    "                list_idx.append(DCN(torch.einsum(\"pq,p->q\", (gs_matching_mat, target_idx))))\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Permute sequences of input_left.\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                # input_left_ord = MSA_inverse_permutation(input_left, gs_mat)\n",
    "                input_left_ord = MSA_inverse_permutation(input_left, gs_matching_mat)\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                ## Get output logits of MSA Transformer for the permuted, masked input\n",
    "                # ----------------------------------------------------------------------------------\n",
    "                loss = torch.tensor(0., device=self.device)\n",
    "                input_for_CE = input_left_ord if self.only_mask_left else torch.cat((input_left_ord, input_right[..., 1:, :]), dim=2)\n",
    "                for p_mask in self._p_mask:\n",
    "                    logits, mask = self(\n",
    "                        input_left_ord, input_right, positive_examples=positive_examples,\n",
    "                        p_mask=p_mask, offset_pos_emb_from=offset_pos_emb_from,\n",
    "                        offset_pos_emb_by=offset_pos_emb_by)\n",
    "                    # MLM loss\n",
    "                    loss = loss + self.Loss(logits[mask], input_for_CE[..., 1:, :][mask])\n",
    "                loss = loss / batch_size\n",
    "                pure_loss = loss.item()\n",
    "                # Save loss values\n",
    "                losses.append(pure_loss * batch_size)\n",
    "                # Update log_alpha\n",
    "                if i < iterations:\n",
    "                    if ((i + 1) % batch_size == 0) or ((i + 1) == iterations):                \n",
    "                        self._init_log_alpha()\n",
    "                        self.log_alpha = [elem.requires_grad_(False) for elem in self.log_alpha]\n",
    "\n",
    "                #      plot and save at every batch_size     or       no plots and save at last iteration\n",
    "                if (((i + 1) % batch_size == 0) and showfig) or ((i == iterations + batch_size - 1) and not showfig):\n",
    "                    self.plot_real_time(\n",
    "                        i, DCN(gs_matching_mat), DCN(gs_mat), list_idx, target_idx_np,\n",
    "                        list_log_alpha, losses, batch_size, epochs,\n",
    "                        0, tar_loss, new_noise_factor, folder_path,\n",
    "                        showfig, only_loss_plot)\n",
    "\n",
    "        return losses, None, [target_idx_np, list_idx], [mats, mats_gs], list_log_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Permutations_MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# !nbdev_export\n",
    "# !nbdev_test\n",
    "# !nbdev_clean\n",
    "# !nbdev_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
