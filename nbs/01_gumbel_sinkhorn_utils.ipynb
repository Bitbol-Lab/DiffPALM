{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gumbel_sinkhorn_utils\n",
    "\n",
    "> Gumbel-Sinkhorn and Gumbel-matching operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp gumbel_sinkhorn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Modified from: https://github.com/perrying/gumbel-sinkhorn\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def sample_uniform(log_alpha_size: torch.Size):\n",
    "    return torch.rand(log_alpha_size)\n",
    "\n",
    "\n",
    "def sinkhorn_norm(alpha: torch.Tensor, n_iter: int = 20) -> Tuple[torch.Tensor,]:\n",
    "    for _ in range(n_iter):\n",
    "        alpha = alpha / alpha.sum(-1, keepdim=True)\n",
    "        alpha = alpha / alpha.sum(-2, keepdim=True)\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def log_sinkhorn_norm(\n",
    "    log_alpha: torch.Tensor, n_iter: int = 20\n",
    ") -> Tuple[torch.Tensor,]:\n",
    "    for _ in range(n_iter):\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -1, keepdim=True)\n",
    "        log_alpha = log_alpha - torch.logsumexp(log_alpha, -2, keepdim=True)\n",
    "    return log_alpha.exp()\n",
    "\n",
    "\n",
    "def gumbel_sinkhorn(\n",
    "    log_alpha: torch.Tensor,\n",
    "    noise_mat: torch.Tensor = 0,\n",
    "    tau: float = 1.0,\n",
    "    n_iter: int = 20,\n",
    "    noise: bool = True,\n",
    "    noise_factor: float = 1.0,\n",
    "    noise_std: bool = False,\n",
    "    rand_perm=None,\n",
    ") -> Tuple[torch.Tensor,]:\n",
    "    if noise:\n",
    "        if noise_std:\n",
    "            noise_factor = noise_factor * torch.std(log_alpha)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise_mat + 1e-20) + 1e-20)\n",
    "        log_alpha = log_alpha + gumbel_noise * noise_factor\n",
    "    log_alpha = log_alpha / tau\n",
    "    sampled_perm_mat = log_sinkhorn_norm(log_alpha, n_iter)\n",
    "    return sampled_perm_mat\n",
    "\n",
    "\n",
    "def gen_assignment(cost_matrix):\n",
    "    row, col = linear_sum_assignment(cost_matrix, maximize=True)\n",
    "    np_assignment_matrix = np.zeros_like(cost_matrix)\n",
    "    np_assignment_matrix[row, col] = 1\n",
    "    return np_assignment_matrix\n",
    "\n",
    "\n",
    "def gumbel_matching(\n",
    "    log_alpha: torch.Tensor,\n",
    "    noise_mat: torch.Tensor = 0,\n",
    "    noise: bool = True,\n",
    "    noise_factor: float = 1.0,\n",
    "    noise_std: bool = False,\n",
    "    rand_perm=None,\n",
    ") -> Tuple[torch.Tensor,]:\n",
    "    if noise:\n",
    "        if noise_std:\n",
    "            noise_factor = noise_factor * torch.std(log_alpha)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise_mat + 1e-20) + 1e-20)\n",
    "        log_alpha = log_alpha + gumbel_noise * noise_factor\n",
    "    if rand_perm is not None:\n",
    "        log_alpha = rand_perm[0] @ log_alpha @ rand_perm[1].T\n",
    "    np_log_alpha = log_alpha.detach().to(\"cpu\").numpy()\n",
    "    np_assignment_mat = gen_assignment(np_log_alpha)\n",
    "    assignment_mat = torch.from_numpy(np_assignment_mat).float().to(log_alpha.device)\n",
    "    if rand_perm is not None:\n",
    "        assignment_mat = rand_perm[0].T @ assignment_mat @ rand_perm[1]\n",
    "    return assignment_mat\n",
    "\n",
    "\n",
    "def MSA_inverse_permutation(X, permutation_matrix):\n",
    "    return torch.einsum(\"pq,bprs->bqrs\", (permutation_matrix, X))\n",
    "\n",
    "\n",
    "def MSA_inverse_permutation_batch(X, permutation_matrices):\n",
    "    return torch.einsum(\"bpq,prs->bqrs\", (permutation_matrices, X))\n",
    "\n",
    "\n",
    "def inverse_permutation(X, permutation_matrix):\n",
    "    return torch.einsum(\"pq,pr->qr\", (permutation_matrix, X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
